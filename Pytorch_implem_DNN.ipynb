{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"prepared_stock_data\"\n",
    "# for i, stock in enumerate(listdir(folder)):\n",
    "#     try:\n",
    "#         if i == 0:\n",
    "#             data = pd.read_csv(folder + \"//\" + stock)\n",
    "#         else:\n",
    "#             data = data.append(pd.read_csv(folder + \"//\" + stock))\n",
    "#     except:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(\"stocks_prepared_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"stocks_prepared_dataset.csv\").drop([\"Unnamed: 0\",\"Name\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>...</th>\n",
       "      <th>T23</th>\n",
       "      <th>T24</th>\n",
       "      <th>T25</th>\n",
       "      <th>T26</th>\n",
       "      <th>T27</th>\n",
       "      <th>T28</th>\n",
       "      <th>T29</th>\n",
       "      <th>T30</th>\n",
       "      <th>T31</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-16</td>\n",
       "      <td>1.250608</td>\n",
       "      <td>1.247874</td>\n",
       "      <td>1.109464</td>\n",
       "      <td>1.104664</td>\n",
       "      <td>1.635328</td>\n",
       "      <td>1.627841</td>\n",
       "      <td>1.720588</td>\n",
       "      <td>1.773613</td>\n",
       "      <td>1.794562</td>\n",
       "      <td>...</td>\n",
       "      <td>1.300995</td>\n",
       "      <td>1.111872</td>\n",
       "      <td>0.991389</td>\n",
       "      <td>0.631393</td>\n",
       "      <td>0.768642</td>\n",
       "      <td>0.216305</td>\n",
       "      <td>0.093381</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>0.029494</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-12-17</td>\n",
       "      <td>1.208991</td>\n",
       "      <td>1.072976</td>\n",
       "      <td>1.068259</td>\n",
       "      <td>1.589744</td>\n",
       "      <td>1.582386</td>\n",
       "      <td>1.673529</td>\n",
       "      <td>1.725637</td>\n",
       "      <td>1.746224</td>\n",
       "      <td>1.849530</td>\n",
       "      <td>...</td>\n",
       "      <td>1.208991</td>\n",
       "      <td>1.104167</td>\n",
       "      <td>0.938166</td>\n",
       "      <td>0.608850</td>\n",
       "      <td>0.607427</td>\n",
       "      <td>0.172903</td>\n",
       "      <td>0.095181</td>\n",
       "      <td>0.027119</td>\n",
       "      <td>-0.023631</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-12-18</td>\n",
       "      <td>1.068415</td>\n",
       "      <td>1.063709</td>\n",
       "      <td>1.584046</td>\n",
       "      <td>1.576705</td>\n",
       "      <td>1.667647</td>\n",
       "      <td>1.719640</td>\n",
       "      <td>1.740181</td>\n",
       "      <td>1.843260</td>\n",
       "      <td>1.949593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119159</td>\n",
       "      <td>1.169856</td>\n",
       "      <td>0.837893</td>\n",
       "      <td>0.609583</td>\n",
       "      <td>0.587052</td>\n",
       "      <td>0.169568</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-12-21</td>\n",
       "      <td>1.026166</td>\n",
       "      <td>1.537037</td>\n",
       "      <td>1.529830</td>\n",
       "      <td>1.619118</td>\n",
       "      <td>1.670165</td>\n",
       "      <td>1.690332</td>\n",
       "      <td>1.791536</td>\n",
       "      <td>1.895935</td>\n",
       "      <td>1.948675</td>\n",
       "      <td>...</td>\n",
       "      <td>1.058960</td>\n",
       "      <td>1.083041</td>\n",
       "      <td>0.719112</td>\n",
       "      <td>0.614687</td>\n",
       "      <td>0.515745</td>\n",
       "      <td>0.130794</td>\n",
       "      <td>0.099383</td>\n",
       "      <td>-0.004472</td>\n",
       "      <td>-0.020352</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-12-22</td>\n",
       "      <td>1.508547</td>\n",
       "      <td>1.501420</td>\n",
       "      <td>1.589706</td>\n",
       "      <td>1.640180</td>\n",
       "      <td>1.660121</td>\n",
       "      <td>1.760188</td>\n",
       "      <td>1.863415</td>\n",
       "      <td>1.915563</td>\n",
       "      <td>1.949749</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071765</td>\n",
       "      <td>0.969799</td>\n",
       "      <td>0.636617</td>\n",
       "      <td>0.587917</td>\n",
       "      <td>0.498723</td>\n",
       "      <td>0.125240</td>\n",
       "      <td>0.100625</td>\n",
       "      <td>0.022055</td>\n",
       "      <td>-0.029217</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0.1        T1        T2        T3        T4        T5        T6  \\\n",
       "0   2009-12-16  1.250608  1.247874  1.109464  1.104664  1.635328  1.627841   \n",
       "1   2009-12-17  1.208991  1.072976  1.068259  1.589744  1.582386  1.673529   \n",
       "2   2009-12-18  1.068415  1.063709  1.584046  1.576705  1.667647  1.719640   \n",
       "3   2009-12-21  1.026166  1.537037  1.529830  1.619118  1.670165  1.690332   \n",
       "4   2009-12-22  1.508547  1.501420  1.589706  1.640180  1.660121  1.760188   \n",
       "\n",
       "         T7        T8        T9  ...       T23       T24       T25       T26  \\\n",
       "0  1.720588  1.773613  1.794562  ...  1.300995  1.111872  0.991389  0.631393   \n",
       "1  1.725637  1.746224  1.849530  ...  1.208991  1.104167  0.938166  0.608850   \n",
       "2  1.740181  1.843260  1.949593  ...  1.119159  1.169856  0.837893  0.609583   \n",
       "3  1.791536  1.895935  1.948675  ...  1.058960  1.083041  0.719112  0.614687   \n",
       "4  1.863415  1.915563  1.949749  ...  1.071765  0.969799  0.636617  0.587917   \n",
       "\n",
       "        T27       T28       T29       T30       T31  Label  \n",
       "0  0.768642  0.216305  0.093381  0.089517  0.029494    NaN  \n",
       "1  0.607427  0.172903  0.095181  0.027119 -0.023631    NaN  \n",
       "2  0.587052  0.169568  0.079762  0.010022 -0.019459    NaN  \n",
       "3  0.515745  0.130794  0.099383 -0.004472 -0.020352    NaN  \n",
       "4  0.498723  0.125240  0.100625  0.022055 -0.029217    NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247620, 33) (124695, 33)\n"
     ]
    }
   ],
   "source": [
    "START = \"2017-01-01\"\n",
    "MIDDLE = \"2019-01-01\"\n",
    "END = \"2020-01-01\"\n",
    "training_data = data[(data[\"Unnamed: 0.1\"] >= START) & (data[\"Unnamed: 0.1\"] < MIDDLE)]\n",
    "testing_data = data[(data[\"Unnamed: 0.1\"] >= MIDDLE) & (data[\"Unnamed: 0.1\"] < END)]\n",
    "print(training_data.shape, testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247510, 33) (124522, 33)\n"
     ]
    }
   ],
   "source": [
    "training_data = training_data.dropna()\n",
    "testing_data = testing_data.dropna()\n",
    "print(training_data.shape, testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>...</th>\n",
       "      <th>T23</th>\n",
       "      <th>T24</th>\n",
       "      <th>T25</th>\n",
       "      <th>T26</th>\n",
       "      <th>T27</th>\n",
       "      <th>T28</th>\n",
       "      <th>T29</th>\n",
       "      <th>T30</th>\n",
       "      <th>T31</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>0.478361</td>\n",
       "      <td>0.468195</td>\n",
       "      <td>0.439730</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.432646</td>\n",
       "      <td>0.457089</td>\n",
       "      <td>0.461161</td>\n",
       "      <td>0.423966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257732</td>\n",
       "      <td>0.077285</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>-0.026448</td>\n",
       "      <td>0.038003</td>\n",
       "      <td>0.077693</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>-0.013401</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0.473238</td>\n",
       "      <td>0.444675</td>\n",
       "      <td>0.440224</td>\n",
       "      <td>0.455884</td>\n",
       "      <td>0.437566</td>\n",
       "      <td>0.462094</td>\n",
       "      <td>0.466180</td>\n",
       "      <td>0.428857</td>\n",
       "      <td>0.430288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241752</td>\n",
       "      <td>0.083765</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>-0.016677</td>\n",
       "      <td>0.028485</td>\n",
       "      <td>0.080895</td>\n",
       "      <td>0.072060</td>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.004183</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0.460185</td>\n",
       "      <td>0.455686</td>\n",
       "      <td>0.471514</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.477791</td>\n",
       "      <td>0.481921</td>\n",
       "      <td>0.444197</td>\n",
       "      <td>0.445644</td>\n",
       "      <td>0.439557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268440</td>\n",
       "      <td>0.092132</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>-0.009718</td>\n",
       "      <td>0.060875</td>\n",
       "      <td>0.102886</td>\n",
       "      <td>0.088016</td>\n",
       "      <td>-0.043651</td>\n",
       "      <td>0.014208</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.482577</td>\n",
       "      <td>0.498697</td>\n",
       "      <td>0.479841</td>\n",
       "      <td>0.505090</td>\n",
       "      <td>0.509296</td>\n",
       "      <td>0.470876</td>\n",
       "      <td>0.472349</td>\n",
       "      <td>0.466150</td>\n",
       "      <td>0.524863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277158</td>\n",
       "      <td>0.112447</td>\n",
       "      <td>0.050367</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.087621</td>\n",
       "      <td>0.090804</td>\n",
       "      <td>0.150909</td>\n",
       "      <td>0.043986</td>\n",
       "      <td>0.029407</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>0.490429</td>\n",
       "      <td>0.471677</td>\n",
       "      <td>0.496787</td>\n",
       "      <td>0.500970</td>\n",
       "      <td>0.462761</td>\n",
       "      <td>0.464226</td>\n",
       "      <td>0.458061</td>\n",
       "      <td>0.516451</td>\n",
       "      <td>0.559585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287258</td>\n",
       "      <td>0.110089</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>-0.007055</td>\n",
       "      <td>0.118107</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.135571</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1        T1        T2        T3        T4        T5        T6  \\\n",
       "1773   2017-01-03  0.478361  0.468195  0.439730  0.435294  0.450900  0.432646   \n",
       "1774   2017-01-04  0.473238  0.444675  0.440224  0.455884  0.437566  0.462094   \n",
       "1775   2017-01-05  0.460185  0.455686  0.471514  0.453000  0.477791  0.481921   \n",
       "1776   2017-01-06  0.482577  0.498697  0.479841  0.505090  0.509296  0.470876   \n",
       "1777   2017-01-09  0.490429  0.471677  0.496787  0.500970  0.462761  0.464226   \n",
       "\n",
       "            T7        T8        T9  ...       T23       T24       T25  \\\n",
       "1773  0.457089  0.461161  0.423966  ...  0.257732  0.077285  0.016667   \n",
       "1774  0.466180  0.428857  0.430288  ...  0.241752  0.083765  0.012604   \n",
       "1775  0.444197  0.445644  0.439557  ...  0.268440  0.092132  0.027076   \n",
       "1776  0.472349  0.466150  0.524863  ...  0.277158  0.112447  0.050367   \n",
       "1777  0.458061  0.516451  0.559585  ...  0.287258  0.110089  0.024725   \n",
       "\n",
       "           T26       T27       T28       T29       T30       T31  Label  \n",
       "1773 -0.026448  0.038003  0.077693  0.055103 -0.013401  0.005061  False  \n",
       "1774 -0.016677  0.028485  0.080895  0.072060 -0.010660 -0.004183   True  \n",
       "1775 -0.009718  0.060875  0.102886  0.088016 -0.043651  0.014208   True  \n",
       "1776  0.002424  0.087621  0.090804  0.150909  0.043986  0.029407   True  \n",
       "1777 -0.007055  0.118107  0.089096  0.135571  0.024326  0.012854  False  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x == False:\n",
    "        return 0\n",
    "    return 1\n",
    "columns = [\"T\"+str(i) for i in range(1,32)]\n",
    "Y_training = training_data['Label'].apply(lambda x: f(x))\n",
    "Y_testing = testing_data['Label'].apply(lambda x: f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.tensor(training_data[columns].values).float()\n",
    "        self.y = torch.tensor(Y_training.values).long()\n",
    "        self.len = self.x.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class TestingData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.tensor(testing_data[columns].values).float()\n",
    "        self.y = torch.tensor(Y_testing.values).long()\n",
    "        self.len = self.x.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Model1(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out=2):\n",
    "        super(DNN_Model1, self).__init__()\n",
    "        self.drop1 = nn.Dropout(p = 0.1)\n",
    "        self.drop2 = nn.Dropout(p = 0.5)\n",
    "        self.channel1_l1 = nn.Linear(D_in, H1)\n",
    "        self.channel1_l2 = nn.Linear(H1, H2)\n",
    "        self.channel1_l3 = nn.Linear(H2, H3)\n",
    "        \n",
    "        self.channel2_l1 = nn.Linear(D_in, H1)\n",
    "        self.channel2_l2 = nn.Linear(H1, H2)\n",
    "        self.channel2_l3 = nn.Linear(H2, H3)\n",
    "        \n",
    "        self.out = nn.Linear(H3, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.max(self.drop1(self.channel1_l1(x)), self.drop1(self.channel2_l1(x)))\n",
    "        x = torch.max(self.drop2(self.channel1_l2(x)), self.drop2(self.channel2_l2(x)))\n",
    "        x = torch.max(self.drop2(self.channel1_l3(x)), self.drop2(self.channel2_l3(x)))\n",
    "        x = F.softmax(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Carlos(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out=2):\n",
    "        super(DNN_Carlos, self).__init__()\n",
    "        self.drop1 = nn.Dropout(p = 0.5)\n",
    "        self.drop2 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.linear1_1 = nn.Linear(D_in, H1)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1_1.weight, nonlinearity = \"leaky_relu\")\n",
    "        self.linear2_1 = nn.Linear(H1, H2)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear2_1.weight, nonlinearity = \"leaky_relu\")\n",
    "        self.linear3_1 = nn.Linear(H2, H3)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear3_1.weight, nonlinearity = \"leaky_relu\")\n",
    "        self.linear1_2 = nn.Linear(D_in, H1)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1_2.weight, nonlinearity = \"leaky_relu\")\n",
    "        self.linear2_2 = nn.Linear(H1, H2)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear2_2.weight, nonlinearity = \"leaky_relu\")\n",
    "        self.linear3_2 = nn.Linear(H2, H3)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear3_2.weight, nonlinearity = \"leaky_relu\")\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(H1)\n",
    "        self.bn2 = nn.BatchNorm1d(H2)\n",
    "        self.bn3 = nn.BatchNorm1d(H3)\n",
    "        \n",
    "        self.out = nn.Linear(H3, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.max(self.bn1(self.drop1(self.linear1_1(x))), self.bn1(self.drop1(self.linear1_2(x))))\n",
    "        x = torch.max(self.bn2(self.drop2(self.linear2_1(x))), self.bn2(self.drop2(self.linear2_2(x))))\n",
    "        x = torch.max(self.bn3(self.linear3_1(x)), self.bn3(self.linear3_2(x)))\n",
    "        x = F.softmax(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    training_info = {'training_loss':[], 'validation_accuracy': []}\n",
    "    MIN = 9999\n",
    "    COUNTER = 0\n",
    "    temp = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            lambda1 = 0.00001\n",
    "            all_params = torch.cat([b.view(-1) for b in model.parameters()])\n",
    "            l1_regularization = lambda1 * torch.norm(all_params, 1)\n",
    "            loss = criterion(y_hat,y) + l1_regularization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_info['training_loss'].append(loss.data.item())\n",
    "        \n",
    "        if MIN == 9999 or loss.data.item() < MIN or epoch < 400:\n",
    "            MIN = loss.data.item()\n",
    "            COUNTER = 0\n",
    "        else:\n",
    "            COUNTER += 1\n",
    "            \n",
    "        correct = 0\n",
    "        accuracy = 0\n",
    "        for x, y in validation_loader:\n",
    "            z = model(x)\n",
    "            _, y_hat = torch.max(z,1)\n",
    "            correct = (y_hat == y).sum().item()\n",
    "            accuracy += correct/x.shape[0]\n",
    "        training_info['validation_accuracy'].append(accuracy/len(validation_loader))\n",
    "        \n",
    "        if COUNTER == 5:\n",
    "            print(\"FINISHED!!!\")\n",
    "            print(\"LOSS:\",training_info['training_loss'][-1],\"ACCURCY:\",training_info['validation_accuracy'][-1])\n",
    "            print(\"current epoch\",epoch+1,\", \", 100*(epoch+1)/epochs, \"% completed\")\n",
    "            break\n",
    "        \n",
    "        print(\"LOSS:\",training_info['training_loss'][-1],\"ACCURCY:\",training_info['validation_accuracy'][-1])\n",
    "        print(\"current epoch\",epoch+1,\", \", 100*(epoch+1)/epochs, \"% completed\")\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    return training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMyModel(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    training_info = {'training_loss':[], 'validation_accuracy': []}\n",
    "    MIN = 9999\n",
    "    COUNTER = 0\n",
    "    temp = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_info['training_loss'].append(loss.data.item())\n",
    "            \n",
    "        correct = 0\n",
    "        accuracy = 0\n",
    "        for x, y in validation_loader:\n",
    "            z = model(x)\n",
    "            _, y_hat = torch.max(z,1)\n",
    "            correct = (y_hat == y).sum().item()\n",
    "            accuracy += correct/x.shape[0]\n",
    "        training_info['validation_accuracy'].append(accuracy/len(validation_loader))\n",
    "        \n",
    "        print(\"LOSS:\",training_info['training_loss'][-1],\"ACCURCY:\",training_info['validation_accuracy'][-1])\n",
    "        print(\"current epoch\",epoch+1,\", \", 100*(epoch+1)/epochs, \"% completed\")\n",
    "        \n",
    "    return training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 31\n",
    "hidden1 = 31\n",
    "hidden2 = 10\n",
    "hidden3 = 5\n",
    "out_dim = 2\n",
    "\n",
    "model1 = DNN_Model1(input_dim, hidden1, hidden2, hidden3, out_dim)\n",
    "#model1.to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TrainingData()\n",
    "testing = TestingData()\n",
    "train_loader = DataLoader(dataset = training, batch_size=2000, shuffle = True)\n",
    "validation_loader = DataLoader(dataset = testing, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1_trainingInfo = train(model1, criterion, train_loader, validation_loader, optimizer, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 trading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = int(len(model1_trainingInfo[\"training_loss\"])/len(model1_trainingInfo[\"validation_accuracy\"]))\n",
    "plt.plot(model1_trainingInfo[\"training_loss\"][::steps], color = \"red\")\n",
    "plt.plot(model1_trainingInfo[\"validation_accuracy\"], color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models//model1_2019.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), path)\n",
    "model1.load_state_dict(torch.load(path))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carlos' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 31\n",
    "hidden1 = 31\n",
    "hidden2 = 10\n",
    "hidden3 = 5\n",
    "out_dim = 2\n",
    "\n",
    "modelCarlos = DNN_Carlos(input_dim, hidden1, hidden2, hidden3, out_dim)\n",
    "#model.to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelCarlos.parameters(), lr=1e-4)\n",
    "\n",
    "training = TrainingData()\n",
    "testing = TestingData()\n",
    "train_loader = DataLoader(dataset = training, batch_size=2000, shuffle = True)\n",
    "validation_loader = DataLoader(dataset = testing, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs582/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.6944779753684998 ACCURCY: 0.515933477222468\n",
      "current epoch 1 ,  0.25 % completed\n",
      "LOSS: 0.696551501750946 ACCURCY: 0.5204229668288369\n",
      "current epoch 2 ,  0.5 % completed\n",
      "LOSS: 0.6915575861930847 ACCURCY: 0.5255465510835914\n",
      "current epoch 3 ,  0.75 % completed\n",
      "LOSS: 0.6886651515960693 ACCURCY: 0.5264334506855374\n",
      "current epoch 4 ,  1.0 % completed\n",
      "LOSS: 0.6886106133460999 ACCURCY: 0.5290707120743035\n",
      "current epoch 5 ,  1.25 % completed\n",
      "LOSS: 0.692809522151947 ACCURCY: 0.5323568332596197\n",
      "current epoch 6 ,  1.5 % completed\n",
      "LOSS: 0.683724045753479 ACCURCY: 0.5347769942503315\n",
      "current epoch 7 ,  1.75 % completed\n",
      "LOSS: 0.6900166869163513 ACCURCY: 0.5341916921716056\n",
      "current epoch 8 ,  2.0 % completed\n",
      "LOSS: 0.6959514617919922 ACCURCY: 0.5384311154356479\n",
      "current epoch 9 ,  2.25 % completed\n",
      "LOSS: 0.6899647116661072 ACCURCY: 0.540888537815126\n",
      "current epoch 10 ,  2.5 % completed\n",
      "LOSS: 0.6902309060096741 ACCURCY: 0.5421096789031402\n",
      "current epoch 11 ,  2.75 % completed\n",
      "LOSS: 0.6830634474754333 ACCURCY: 0.5450875046439629\n",
      "current epoch 12 ,  3.0 % completed\n",
      "LOSS: 0.6815756559371948 ACCURCY: 0.5457232091994692\n",
      "current epoch 13 ,  3.25 % completed\n",
      "LOSS: 0.6852181553840637 ACCURCY: 0.5461074108801415\n",
      "current epoch 14 ,  3.5 % completed\n",
      "LOSS: 0.6802072525024414 ACCURCY: 0.5492871154356479\n",
      "current epoch 15 ,  3.75 % completed\n",
      "LOSS: 0.6820458769798279 ACCURCY: 0.5521356390977443\n",
      "current epoch 16 ,  4.0 % completed\n",
      "LOSS: 0.6777263879776001 ACCURCY: 0.5548973976116763\n",
      "current epoch 17 ,  4.25 % completed\n",
      "LOSS: 0.6835208535194397 ACCURCY: 0.5592880283060592\n",
      "current epoch 18 ,  4.5 % completed\n",
      "LOSS: 0.6801294684410095 ACCURCY: 0.5588683909774437\n",
      "current epoch 19 ,  4.75 % completed\n",
      "LOSS: 0.6694524884223938 ACCURCY: 0.5616384714727995\n",
      "current epoch 20 ,  5.0 % completed\n",
      "LOSS: 0.6781885623931885 ACCURCY: 0.5657456665192392\n",
      "current epoch 21 ,  5.25 % completed\n",
      "LOSS: 0.6738790273666382 ACCURCY: 0.5651807129588677\n",
      "current epoch 22 ,  5.5 % completed\n",
      "LOSS: 0.6644167900085449 ACCURCY: 0.570451103051747\n",
      "current epoch 23 ,  5.75 % completed\n",
      "LOSS: 0.6692070960998535 ACCURCY: 0.5726946731534719\n",
      "current epoch 24 ,  6.0 % completed\n",
      "LOSS: 0.6748228073120117 ACCURCY: 0.5732438142414861\n",
      "current epoch 25 ,  6.25 % completed\n",
      "LOSS: 0.6682625412940979 ACCURCY: 0.5764198142414861\n",
      "current epoch 26 ,  6.5 % completed\n",
      "LOSS: 0.6650214791297913 ACCURCY: 0.579980861565679\n",
      "current epoch 27 ,  6.75 % completed\n",
      "LOSS: 0.6558939218521118 ACCURCY: 0.5803870358248562\n",
      "current epoch 28 ,  7.0 % completed\n",
      "LOSS: 0.6662590503692627 ACCURCY: 0.5853673312693498\n",
      "current epoch 29 ,  7.25 % completed\n",
      "LOSS: 0.64844810962677 ACCURCY: 0.5884367271118974\n",
      "current epoch 30 ,  7.5 % completed\n",
      "LOSS: 0.6566347479820251 ACCURCY: 0.5888652242370632\n",
      "current epoch 31 ,  7.75 % completed\n",
      "LOSS: 0.6450368165969849 ACCURCY: 0.5944372507739938\n",
      "current epoch 32 ,  8.0 % completed\n",
      "LOSS: 0.6572781205177307 ACCURCY: 0.5959642176028307\n",
      "current epoch 33 ,  8.25 % completed\n",
      "LOSS: 0.6549355387687683 ACCURCY: 0.598441291463954\n",
      "current epoch 34 ,  8.5 % completed\n",
      "LOSS: 0.6530748605728149 ACCURCY: 0.6013039893852277\n",
      "current epoch 35 ,  8.75 % completed\n",
      "LOSS: 0.6587862968444824 ACCURCY: 0.6017070499778858\n",
      "current epoch 36 ,  9.0 % completed\n",
      "LOSS: 0.648742139339447 ACCURCY: 0.6063163520566123\n",
      "current epoch 37 ,  9.25 % completed\n",
      "LOSS: 0.6517408490180969 ACCURCY: 0.6076561910659001\n",
      "current epoch 38 ,  9.5 % completed\n",
      "LOSS: 0.6429939866065979 ACCURCY: 0.6115508889871737\n",
      "current epoch 39 ,  9.75 % completed\n",
      "LOSS: 0.6428686380386353 ACCURCY: 0.6123127147279965\n",
      "current epoch 40 ,  10.0 % completed\n",
      "LOSS: 0.6457506418228149 ACCURCY: 0.616664647501106\n",
      "current epoch 41 ,  10.25 % completed\n",
      "LOSS: 0.6459124088287354 ACCURCY: 0.6176710906678461\n",
      "current epoch 42 ,  10.5 % completed\n",
      "LOSS: 0.6437234282493591 ACCURCY: 0.6193659230429015\n",
      "current epoch 43 ,  10.75 % completed\n",
      "LOSS: 0.6422722339630127 ACCURCY: 0.6214735603715168\n",
      "current epoch 44 ,  11.0 % completed\n",
      "LOSS: 0.6469605565071106 ACCURCY: 0.6250483927465723\n",
      "current epoch 45 ,  11.25 % completed\n",
      "LOSS: 0.6259707808494568 ACCURCY: 0.6260021512605041\n",
      "current epoch 46 ,  11.5 % completed\n",
      "LOSS: 0.6381768584251404 ACCURCY: 0.6281000973020787\n",
      "current epoch 47 ,  11.75 % completed\n",
      "LOSS: 0.6530808806419373 ACCURCY: 0.6297976010614773\n",
      "current epoch 48 ,  12.0 % completed\n",
      "LOSS: 0.637755274772644 ACCURCY: 0.6331177355152587\n",
      "current epoch 49 ,  12.25 % completed\n",
      "LOSS: 0.6278586387634277 ACCURCY: 0.6356493728438745\n",
      "current epoch 50 ,  12.5 % completed\n",
      "LOSS: 0.6316599249839783 ACCURCY: 0.6352989969040248\n",
      "current epoch 51 ,  12.75 % completed\n",
      "LOSS: 0.6391795873641968 ACCURCY: 0.637847037593985\n",
      "current epoch 52 ,  13.0 % completed\n",
      "LOSS: 0.624687671661377 ACCURCY: 0.639985118089341\n",
      "current epoch 53 ,  13.25 % completed\n",
      "LOSS: 0.6300966143608093 ACCURCY: 0.6419724334365324\n",
      "current epoch 54 ,  13.5 % completed\n",
      "LOSS: 0.6262604594230652 ACCURCY: 0.6435514940291907\n",
      "current epoch 55 ,  13.75 % completed\n",
      "LOSS: 0.6284891366958618 ACCURCY: 0.6437907156125607\n",
      "current epoch 56 ,  14.0 % completed\n",
      "LOSS: 0.6390706896781921 ACCURCY: 0.6460660309597523\n",
      "current epoch 57 ,  14.25 % completed\n",
      "LOSS: 0.616001307964325 ACCURCY: 0.6473904608580273\n",
      "current epoch 58 ,  14.5 % completed\n",
      "LOSS: 0.6190705895423889 ACCURCY: 0.6472969305616985\n",
      "current epoch 59 ,  14.75 % completed\n",
      "LOSS: 0.6141480207443237 ACCURCY: 0.64864346749226\n",
      "current epoch 60 ,  15.0 % completed\n",
      "LOSS: 0.6366727352142334 ACCURCY: 0.6518215479876162\n",
      "current epoch 61 ,  15.25 % completed\n",
      "LOSS: 0.6118767857551575 ACCURCY: 0.6503743936311366\n",
      "current epoch 62 ,  15.5 % completed\n",
      "LOSS: 0.6308683753013611 ACCURCY: 0.6508409305616983\n",
      "current epoch 63 ,  15.75 % completed\n",
      "LOSS: 0.623015820980072 ACCURCY: 0.6531907156125608\n",
      "current epoch 64 ,  16.0 % completed\n",
      "LOSS: 0.6328061819076538 ACCURCY: 0.6540607023440955\n",
      "current epoch 65 ,  16.25 % completed\n",
      "LOSS: 0.6027325391769409 ACCURCY: 0.6539883131357805\n",
      "current epoch 66 ,  16.5 % completed\n",
      "LOSS: 0.6267601847648621 ACCURCY: 0.6552881521450685\n",
      "current epoch 67 ,  16.75 % completed\n",
      "LOSS: 0.6371645331382751 ACCURCY: 0.6556291587793011\n",
      "current epoch 68 ,  17.0 % completed\n",
      "LOSS: 0.627278208732605 ACCURCY: 0.6571942998673154\n",
      "current epoch 69 ,  17.25 % completed\n",
      "LOSS: 0.6201384663581848 ACCURCY: 0.6568663670942061\n",
      "current epoch 70 ,  17.5 % completed\n",
      "LOSS: 0.6408539414405823 ACCURCY: 0.6570486085802743\n",
      "current epoch 71 ,  17.75 % completed\n",
      "LOSS: 0.6161406636238098 ACCURCY: 0.6595118301636443\n",
      "current epoch 72 ,  18.0 % completed\n",
      "LOSS: 0.6254546046257019 ACCURCY: 0.6596888102609466\n",
      "current epoch 73 ,  18.25 % completed\n",
      "LOSS: 0.6081240177154541 ACCURCY: 0.6594153737284387\n",
      "current epoch 74 ,  18.5 % completed\n",
      "LOSS: 0.6309229135513306 ACCURCY: 0.6604731322423707\n",
      "current epoch 75 ,  18.75 % completed\n",
      "LOSS: 0.6250757575035095 ACCURCY: 0.6597412932330827\n",
      "current epoch 76 ,  19.0 % completed\n",
      "LOSS: 0.6026612520217896 ACCURCY: 0.6599561256081381\n",
      "current epoch 77 ,  19.25 % completed\n",
      "LOSS: 0.6171672940254211 ACCURCY: 0.6633248367978771\n",
      "current epoch 78 ,  19.5 % completed\n",
      "LOSS: 0.5996228456497192 ACCURCY: 0.6614317629367537\n",
      "current epoch 79 ,  19.75 % completed\n",
      "LOSS: 0.6170860528945923 ACCURCY: 0.6623560583812472\n",
      "current epoch 80 ,  20.0 % completed\n",
      "LOSS: 0.5986415147781372 ACCURCY: 0.6627147563025211\n",
      "current epoch 81 ,  20.25 % completed\n",
      "LOSS: 0.6211926937103271 ACCURCY: 0.6630123803626715\n",
      "current epoch 82 ,  20.5 % completed\n",
      "LOSS: 0.6040152311325073 ACCURCY: 0.6625742733303848\n",
      "current epoch 83 ,  20.75 % completed\n",
      "LOSS: 0.6182234287261963 ACCURCY: 0.6635911862007958\n",
      "current epoch 84 ,  21.0 % completed\n",
      "LOSS: 0.6185176372528076 ACCURCY: 0.6643716691729324\n",
      "current epoch 85 ,  21.25 % completed\n",
      "LOSS: 0.6076708436012268 ACCURCY: 0.6649984077841663\n",
      "current epoch 86 ,  21.5 % completed\n",
      "LOSS: 0.6165223717689514 ACCURCY: 0.6666769040247678\n",
      "current epoch 87 ,  21.75 % completed\n",
      "LOSS: 0.6175628304481506 ACCURCY: 0.6661957363998231\n",
      "current epoch 88 ,  22.0 % completed\n",
      "LOSS: 0.6092894673347473 ACCURCY: 0.6646773604599734\n",
      "current epoch 89 ,  22.25 % completed\n",
      "LOSS: 0.6105093955993652 ACCURCY: 0.6651469579831933\n",
      "current epoch 90 ,  22.5 % completed\n",
      "LOSS: 0.6037413477897644 ACCURCY: 0.6657351189739054\n",
      "current epoch 91 ,  22.75 % completed\n",
      "LOSS: 0.6102221608161926 ACCURCY: 0.6650959646174259\n",
      "current epoch 92 ,  23.0 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.6302527785301208 ACCURCY: 0.6678534949137551\n",
      "current epoch 93 ,  23.25 % completed\n",
      "LOSS: 0.5931487679481506 ACCURCY: 0.6655178168951791\n",
      "current epoch 94 ,  23.5 % completed\n",
      "LOSS: 0.6122202277183533 ACCURCY: 0.6672311189739052\n",
      "current epoch 95 ,  23.75 % completed\n",
      "LOSS: 0.6263964772224426 ACCURCY: 0.6662395081822201\n",
      "current epoch 96 ,  24.0 % completed\n",
      "LOSS: 0.62004154920578 ACCURCY: 0.6684816559044671\n",
      "current epoch 97 ,  24.25 % completed\n",
      "LOSS: 0.6050337553024292 ACCURCY: 0.6677123538257409\n",
      "current epoch 98 ,  24.5 % completed\n",
      "LOSS: 0.5930981636047363 ACCURCY: 0.6675081928350288\n",
      "current epoch 99 ,  24.75 % completed\n",
      "LOSS: 0.6067746877670288 ACCURCY: 0.6664032799646177\n",
      "current epoch 100 ,  25.0 % completed\n",
      "LOSS: 0.6054993271827698 ACCURCY: 0.6681456559044671\n",
      "current epoch 101 ,  25.25 % completed\n",
      "LOSS: 0.6040242910385132 ACCURCY: 0.6690709845201238\n",
      "current epoch 102 ,  25.5 % completed\n",
      "LOSS: 0.6037255525588989 ACCURCY: 0.6674778841220699\n",
      "current epoch 103 ,  25.75 % completed\n",
      "LOSS: 0.5958656668663025 ACCURCY: 0.6703120318443166\n",
      "current epoch 104 ,  26.0 % completed\n",
      "LOSS: 0.6089128851890564 ACCURCY: 0.6703094276868642\n",
      "current epoch 105 ,  26.25 % completed\n",
      "LOSS: 0.5926151275634766 ACCURCY: 0.6697044210526316\n",
      "current epoch 106 ,  26.5 % completed\n",
      "LOSS: 0.6080836057662964 ACCURCY: 0.6693637363998229\n",
      "current epoch 107 ,  26.75 % completed\n",
      "LOSS: 0.5926882028579712 ACCURCY: 0.67004750818222\n",
      "current epoch 108 ,  27.0 % completed\n",
      "LOSS: 0.6253206729888916 ACCURCY: 0.6700118036267139\n",
      "current epoch 109 ,  27.25 % completed\n",
      "LOSS: 0.6102418303489685 ACCURCY: 0.6707044882795224\n",
      "current epoch 110 ,  27.5 % completed\n",
      "LOSS: 0.6029478311538696 ACCURCY: 0.6720662467934544\n",
      "current epoch 111 ,  27.75 % completed\n",
      "LOSS: 0.6046191453933716 ACCURCY: 0.6714050384785493\n",
      "current epoch 112 ,  28.0 % completed\n",
      "LOSS: 0.6007143259048462 ACCURCY: 0.6696386625386999\n",
      "current epoch 113 ,  28.25 % completed\n",
      "LOSS: 0.605163037776947 ACCURCY: 0.6709511862007961\n",
      "current epoch 114 ,  28.5 % completed\n",
      "LOSS: 0.6001604795455933 ACCURCY: 0.6698025015479877\n",
      "current epoch 115 ,  28.75 % completed\n",
      "LOSS: 0.617579460144043 ACCURCY: 0.6702189579831931\n",
      "current epoch 116 ,  29.0 % completed\n",
      "LOSS: 0.6037196516990662 ACCURCY: 0.6726467969924812\n",
      "current epoch 117 ,  29.25 % completed\n",
      "LOSS: 0.6086951494216919 ACCURCY: 0.6720336559044671\n",
      "current epoch 118 ,  29.5 % completed\n",
      "LOSS: 0.609804630279541 ACCURCY: 0.67160588412207\n",
      "current epoch 119 ,  29.75 % completed\n",
      "LOSS: 0.6159422993659973 ACCURCY: 0.6723971729323308\n",
      "current epoch 120 ,  30.0 % completed\n",
      "LOSS: 0.609024167060852 ACCURCY: 0.6731174949137549\n",
      "current epoch 121 ,  30.25 % completed\n",
      "LOSS: 0.6032845973968506 ACCURCY: 0.6733634816452897\n",
      "current epoch 122 ,  30.5 % completed\n",
      "LOSS: 0.5955795645713806 ACCURCY: 0.6722623405572754\n",
      "current epoch 123 ,  30.75 % completed\n",
      "LOSS: 0.6269635558128357 ACCURCY: 0.6724980451127817\n",
      "current epoch 124 ,  31.0 % completed\n",
      "LOSS: 0.6040818095207214 ACCURCY: 0.6728606492702345\n",
      "current epoch 125 ,  31.25 % completed\n",
      "LOSS: 0.6206666827201843 ACCURCY: 0.6714000318443166\n",
      "current epoch 126 ,  31.5 % completed\n",
      "LOSS: 0.6051031351089478 ACCURCY: 0.673320555506413\n",
      "current epoch 127 ,  31.75 % completed\n",
      "LOSS: 0.6074008345603943 ACCURCY: 0.6740818575851394\n",
      "current epoch 128 ,  32.0 % completed\n",
      "LOSS: 0.6102985739707947 ACCURCY: 0.6724771057054402\n",
      "current epoch 129 ,  32.25 % completed\n",
      "LOSS: 0.6085420250892639 ACCURCY: 0.6731806492702342\n",
      "current epoch 130 ,  32.5 % completed\n",
      "LOSS: 0.6235564351081848 ACCURCY: 0.6734151862007961\n",
      "current epoch 131 ,  32.75 % completed\n",
      "LOSS: 0.6059989333152771 ACCURCY: 0.67312588412207\n",
      "current epoch 132 ,  33.0 % completed\n",
      "LOSS: 0.5915428996086121 ACCURCY: 0.6754648774878372\n",
      "current epoch 133 ,  33.25 % completed\n",
      "LOSS: 0.5998357534408569 ACCURCY: 0.6747200990712074\n",
      "current epoch 134 ,  33.5 % completed\n",
      "LOSS: 0.5970694422721863 ACCURCY: 0.675025333923043\n",
      "current epoch 135 ,  33.75 % completed\n",
      "LOSS: 0.5948525667190552 ACCURCY: 0.6751393471915081\n",
      "current epoch 136 ,  34.0 % completed\n",
      "LOSS: 0.5989894866943359 ACCURCY: 0.6743007164971252\n",
      "current epoch 137 ,  34.25 % completed\n",
      "LOSS: 0.6088284254074097 ACCURCY: 0.6745634816452896\n",
      "current epoch 138 ,  34.5 % completed\n",
      "LOSS: 0.5989571213722229 ACCURCY: 0.6739533339230429\n",
      "current epoch 139 ,  34.75 % completed\n",
      "LOSS: 0.604958176612854 ACCURCY: 0.6741225687748782\n",
      "current epoch 140 ,  35.0 % completed\n",
      "LOSS: 0.6015350818634033 ACCURCY: 0.6747111862007961\n",
      "current epoch 141 ,  35.25 % completed\n",
      "LOSS: 0.6134618520736694 ACCURCY: 0.6747048774878373\n",
      "current epoch 142 ,  35.5 % completed\n",
      "LOSS: 0.6236041188240051 ACCURCY: 0.6745281662980983\n",
      "current epoch 143 ,  35.75 % completed\n",
      "LOSS: 0.6017988324165344 ACCURCY: 0.6752156426360016\n",
      "current epoch 144 ,  36.0 % completed\n",
      "LOSS: 0.6026127338409424 ACCURCY: 0.6759832534276868\n",
      "current epoch 145 ,  36.25 % completed\n",
      "LOSS: 0.6108551025390625 ACCURCY: 0.6752728774878372\n",
      "current epoch 146 ,  36.5 % completed\n",
      "LOSS: 0.6039194464683533 ACCURCY: 0.6744585687748784\n",
      "current epoch 147 ,  36.75 % completed\n",
      "LOSS: 0.6029825806617737 ACCURCY: 0.6761444882795224\n",
      "current epoch 148 ,  37.0 % completed\n",
      "LOSS: 0.5914821028709412 ACCURCY: 0.6760817231313578\n",
      "current epoch 149 ,  37.25 % completed\n",
      "LOSS: 0.6019251942634583 ACCURCY: 0.6761171729323308\n",
      "current epoch 150 ,  37.5 % completed\n",
      "LOSS: 0.605750322341919 ACCURCY: 0.6759757098628926\n",
      "current epoch 151 ,  37.75 % completed\n",
      "LOSS: 0.5932532548904419 ACCURCY: 0.6758814011499336\n",
      "current epoch 152 ,  38.0 % completed\n",
      "LOSS: 0.6012370586395264 ACCURCY: 0.6764316426360018\n",
      "current epoch 153 ,  38.25 % completed\n",
      "LOSS: 0.5942336916923523 ACCURCY: 0.6753580185758513\n",
      "current epoch 154 ,  38.5 % completed\n",
      "LOSS: 0.597745954990387 ACCURCY: 0.6758484882795224\n",
      "current epoch 155 ,  38.75 % completed\n",
      "LOSS: 0.5963490605354309 ACCURCY: 0.6762851057054401\n",
      "current epoch 156 ,  39.0 % completed\n",
      "LOSS: 0.6017696857452393 ACCURCY: 0.6758400990712075\n",
      "current epoch 157 ,  39.25 % completed\n",
      "LOSS: 0.5984277725219727 ACCURCY: 0.6755360318443167\n",
      "current epoch 158 ,  39.5 % completed\n",
      "LOSS: 0.6018092632293701 ACCURCY: 0.6761478708536046\n",
      "current epoch 159 ,  39.75 % completed\n",
      "LOSS: 0.5986014604568481 ACCURCY: 0.6768472534276868\n",
      "current epoch 160 ,  40.0 % completed\n",
      "LOSS: 0.5962837934494019 ACCURCY: 0.6765837098628925\n",
      "current epoch 161 ,  40.25 % completed\n",
      "LOSS: 0.6056528091430664 ACCURCY: 0.676873723131358\n",
      "current epoch 162 ,  40.5 % completed\n",
      "LOSS: 0.6025997996330261 ACCURCY: 0.6747087837240159\n",
      "current epoch 163 ,  40.75 % completed\n",
      "LOSS: 0.5940175652503967 ACCURCY: 0.6762808774878372\n",
      "current epoch 164 ,  41.0 % completed\n",
      "LOSS: 0.5883346199989319 ACCURCY: 0.6768886227333037\n",
      "current epoch 165 ,  41.25 % completed\n",
      "LOSS: 0.5984394550323486 ACCURCY: 0.6759987969924812\n",
      "current epoch 166 ,  41.5 % completed\n",
      "LOSS: 0.6005253791809082 ACCURCY: 0.6773798036267139\n",
      "current epoch 167 ,  41.75 % completed\n",
      "LOSS: 0.6048357486724854 ACCURCY: 0.677312944714728\n",
      "current epoch 168 ,  42.0 % completed\n",
      "LOSS: 0.5745751857757568 ACCURCY: 0.6771718708536046\n",
      "current epoch 169 ,  42.25 % completed\n",
      "LOSS: 0.5975782871246338 ACCURCY: 0.676894864219372\n",
      "current epoch 170 ,  42.5 % completed\n",
      "LOSS: 0.6054515838623047 ACCURCY: 0.6788977903582486\n",
      "current epoch 171 ,  42.75 % completed\n",
      "LOSS: 0.5899073481559753 ACCURCY: 0.676831253427687\n",
      "current epoch 172 ,  43.0 % completed\n",
      "LOSS: 0.5982627868652344 ACCURCY: 0.6768679380804956\n",
      "current epoch 173 ,  43.25 % completed\n",
      "LOSS: 0.6030948758125305 ACCURCY: 0.6765454011499337\n",
      "current epoch 174 ,  43.5 % completed\n",
      "LOSS: 0.6001237034797668 ACCURCY: 0.6777811729323308\n",
      "current epoch 175 ,  43.75 % completed\n",
      "LOSS: 0.6065475940704346 ACCURCY: 0.6777697231313577\n",
      "current epoch 176 ,  44.0 % completed\n",
      "LOSS: 0.5949633717536926 ACCURCY: 0.6776400990712075\n",
      "current epoch 177 ,  44.25 % completed\n",
      "LOSS: 0.5946069359779358 ACCURCY: 0.6779045555064132\n",
      "current epoch 178 ,  44.5 % completed\n",
      "LOSS: 0.5899274349212646 ACCURCY: 0.6768884210526316\n",
      "current epoch 179 ,  44.75 % completed\n",
      "LOSS: 0.5932698249816895 ACCURCY: 0.6776696965944272\n",
      "current epoch 180 ,  45.0 % completed\n",
      "LOSS: 0.6112370491027832 ACCURCY: 0.6791576293675365\n",
      "current epoch 181 ,  45.25 % completed\n",
      "LOSS: 0.6047843098640442 ACCURCY: 0.6778691057054402\n",
      "current epoch 182 ,  45.5 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.5826140642166138 ACCURCY: 0.677608166298098\n",
      "current epoch 183 ,  45.75 % completed\n",
      "LOSS: 0.6027217507362366 ACCURCY: 0.6782570119416188\n",
      "current epoch 184 ,  46.0 % completed\n",
      "LOSS: 0.5980218648910522 ACCURCY: 0.6787571729323308\n",
      "current epoch 185 ,  46.25 % completed\n",
      "LOSS: 0.5910559892654419 ACCURCY: 0.6794030924369748\n",
      "current epoch 186 ,  46.5 % completed\n",
      "LOSS: 0.6055617332458496 ACCURCY: 0.6787899513489606\n",
      "current epoch 187 ,  46.75 % completed\n",
      "LOSS: 0.6060870289802551 ACCURCY: 0.679115025210084\n",
      "current epoch 188 ,  47.0 % completed\n",
      "LOSS: 0.5886852741241455 ACCURCY: 0.6778540185758515\n",
      "current epoch 189 ,  47.25 % completed\n",
      "LOSS: 0.5825267434120178 ACCURCY: 0.6777833206545776\n",
      "current epoch 190 ,  47.5 % completed\n",
      "LOSS: 0.6086562275886536 ACCURCY: 0.6777878708536046\n",
      "current epoch 191 ,  47.75 % completed\n",
      "LOSS: 0.6041283011436462 ACCURCY: 0.6783463140203448\n",
      "current epoch 192 ,  48.0 % completed\n",
      "LOSS: 0.5933089256286621 ACCURCY: 0.6780165555064133\n",
      "current epoch 193 ,  48.25 % completed\n",
      "LOSS: 0.6040147542953491 ACCURCY: 0.678678864219372\n",
      "current epoch 194 ,  48.5 % completed\n",
      "LOSS: 0.5997372269630432 ACCURCY: 0.6787083945157009\n",
      "current epoch 195 ,  48.75 % completed\n",
      "LOSS: 0.600152850151062 ACCURCY: 0.6780666360017692\n",
      "current epoch 196 ,  49.0 % completed\n",
      "LOSS: 0.5984892249107361 ACCURCY: 0.6783963272888102\n",
      "current epoch 197 ,  49.25 % completed\n",
      "LOSS: 0.6068918704986572 ACCURCY: 0.6795698575851395\n",
      "current epoch 198 ,  49.5 % completed\n",
      "LOSS: 0.5966400504112244 ACCURCY: 0.6793260185758514\n",
      "current epoch 199 ,  49.75 % completed\n",
      "LOSS: 0.5960585474967957 ACCURCY: 0.6778640990712074\n",
      "current epoch 200 ,  50.0 % completed\n",
      "LOSS: 0.6093227863311768 ACCURCY: 0.6790131729323307\n",
      "current epoch 201 ,  50.25 % completed\n",
      "LOSS: 0.5843916535377502 ACCURCY: 0.6782577903582486\n",
      "current epoch 202 ,  50.5 % completed\n",
      "LOSS: 0.5792881846427917 ACCURCY: 0.679674636001769\n",
      "current epoch 203 ,  50.75 % completed\n",
      "LOSS: 0.5957329869270325 ACCURCY: 0.6789062467934542\n",
      "current epoch 204 ,  51.0 % completed\n",
      "LOSS: 0.6030946969985962 ACCURCY: 0.6793547032286598\n",
      "current epoch 205 ,  51.25 % completed\n",
      "LOSS: 0.5942085981369019 ACCURCY: 0.6787268642193719\n",
      "current epoch 206 ,  51.5 % completed\n",
      "LOSS: 0.5991613268852234 ACCURCY: 0.6792426360017692\n",
      "current epoch 207 ,  51.75 % completed\n",
      "LOSS: 0.6066474318504333 ACCURCY: 0.6782957098628924\n",
      "current epoch 208 ,  52.0 % completed\n",
      "LOSS: 0.6070104837417603 ACCURCY: 0.6786207164971251\n",
      "current epoch 209 ,  52.25 % completed\n",
      "LOSS: 0.6010836958885193 ACCURCY: 0.6793592534276869\n",
      "current epoch 210 ,  52.5 % completed\n",
      "LOSS: 0.5844104290008545 ACCURCY: 0.6790135621406457\n",
      "current epoch 211 ,  52.75 % completed\n",
      "LOSS: 0.5862902402877808 ACCURCY: 0.6780237098628926\n",
      "current epoch 212 ,  53.0 % completed\n",
      "LOSS: 0.6016983389854431 ACCURCY: 0.6794168774878373\n",
      "current epoch 213 ,  53.25 % completed\n",
      "LOSS: 0.5963900089263916 ACCURCY: 0.6787331729323308\n",
      "current epoch 214 ,  53.5 % completed\n",
      "LOSS: 0.5825777649879456 ACCURCY: 0.6797411729323308\n",
      "current epoch 215 ,  53.75 % completed\n",
      "LOSS: 0.5918084979057312 ACCURCY: 0.6784810119416188\n",
      "current epoch 216 ,  54.0 % completed\n",
      "LOSS: 0.5971217155456543 ACCURCY: 0.6798953206545777\n",
      "current epoch 217 ,  54.25 % completed\n",
      "LOSS: 0.6026425361633301 ACCURCY: 0.6798397770897833\n",
      "current epoch 218 ,  54.5 % completed\n",
      "LOSS: 0.5980057716369629 ACCURCY: 0.6788523272888102\n",
      "current epoch 219 ,  54.75 % completed\n",
      "LOSS: 0.5885195136070251 ACCURCY: 0.679182864219372\n",
      "current epoch 220 ,  55.0 % completed\n",
      "LOSS: 0.609894335269928 ACCURCY: 0.6798776293675367\n",
      "current epoch 221 ,  55.25 % completed\n",
      "LOSS: 0.6060545444488525 ACCURCY: 0.6790317770897835\n",
      "current epoch 222 ,  55.5 % completed\n",
      "LOSS: 0.5962881445884705 ACCURCY: 0.6784810119416187\n",
      "current epoch 223 ,  55.75 % completed\n",
      "LOSS: 0.5866836905479431 ACCURCY: 0.6793610119416188\n",
      "current epoch 224 ,  56.0 % completed\n",
      "LOSS: 0.5779725909233093 ACCURCY: 0.6793732401592216\n",
      "current epoch 225 ,  56.25 % completed\n",
      "LOSS: 0.6038493514060974 ACCURCY: 0.6791634816452897\n",
      "current epoch 226 ,  56.5 % completed\n",
      "LOSS: 0.6123852133750916 ACCURCY: 0.6799920990712076\n",
      "current epoch 227 ,  56.75 % completed\n",
      "LOSS: 0.6005879044532776 ACCURCY: 0.6799420185758513\n",
      "current epoch 228 ,  57.0 % completed\n",
      "LOSS: 0.5936994552612305 ACCURCY: 0.6794077098628926\n",
      "current epoch 229 ,  57.25 % completed\n",
      "LOSS: 0.5993903875350952 ACCURCY: 0.6795546360017692\n",
      "current epoch 230 ,  57.5 % completed\n",
      "LOSS: 0.5920074582099915 ACCURCY: 0.6802313206545776\n",
      "current epoch 231 ,  57.75 % completed\n",
      "LOSS: 0.5863308906555176 ACCURCY: 0.6794313206545776\n",
      "current epoch 232 ,  58.0 % completed\n",
      "LOSS: 0.5865553617477417 ACCURCY: 0.6797980185758513\n",
      "current epoch 233 ,  58.25 % completed\n",
      "LOSS: 0.595608115196228 ACCURCY: 0.6796843272888101\n",
      "current epoch 234 ,  58.5 % completed\n",
      "LOSS: 0.5946199893951416 ACCURCY: 0.6810283272888102\n",
      "current epoch 235 ,  58.75 % completed\n",
      "LOSS: 0.5991024971008301 ACCURCY: 0.6801950252100842\n",
      "current epoch 236 ,  59.0 % completed\n",
      "LOSS: 0.5855081677436829 ACCURCY: 0.6806750924369749\n",
      "current epoch 237 ,  59.25 % completed\n",
      "LOSS: 0.5820695161819458 ACCURCY: 0.6808363272888103\n",
      "current epoch 238 ,  59.5 % completed\n",
      "LOSS: 0.6061421036720276 ACCURCY: 0.6796645555064132\n",
      "current epoch 239 ,  59.75 % completed\n",
      "LOSS: 0.6079471111297607 ACCURCY: 0.6806716426360014\n",
      "current epoch 240 ,  60.0 % completed\n",
      "LOSS: 0.6086238026618958 ACCURCY: 0.6803206227333038\n",
      "current epoch 241 ,  60.25 % completed\n",
      "LOSS: 0.6066458225250244 ACCURCY: 0.6792662467934543\n",
      "current epoch 242 ,  60.5 % completed\n",
      "LOSS: 0.5966563820838928 ACCURCY: 0.6800107032286598\n",
      "current epoch 243 ,  60.75 % completed\n",
      "LOSS: 0.5897670984268188 ACCURCY: 0.6794598708536046\n",
      "current epoch 244 ,  61.0 % completed\n",
      "LOSS: 0.5990414023399353 ACCURCY: 0.6790334011499337\n",
      "current epoch 245 ,  61.25 % completed\n",
      "LOSS: 0.5846660137176514 ACCURCY: 0.6814797098628926\n",
      "current epoch 246 ,  61.5 % completed\n",
      "LOSS: 0.6132864356040955 ACCURCY: 0.679348783724016\n",
      "current epoch 247 ,  61.75 % completed\n",
      "LOSS: 0.6033593416213989 ACCURCY: 0.6800254011499337\n",
      "current epoch 248 ,  62.0 % completed\n",
      "LOSS: 0.5900563597679138 ACCURCY: 0.6789782467934542\n",
      "current epoch 249 ,  62.25 % completed\n",
      "LOSS: 0.5939668416976929 ACCURCY: 0.6799685555064133\n",
      "current epoch 250 ,  62.5 % completed\n",
      "LOSS: 0.5881591439247131 ACCURCY: 0.6810237098628926\n",
      "current epoch 251 ,  62.75 % completed\n",
      "LOSS: 0.5897760391235352 ACCURCY: 0.6800038708536046\n",
      "current epoch 252 ,  63.0 % completed\n",
      "LOSS: 0.5809906721115112 ACCURCY: 0.6795323272888103\n",
      "current epoch 253 ,  63.25 % completed\n",
      "LOSS: 0.5838580131530762 ACCURCY: 0.6805950924369748\n",
      "current epoch 254 ,  63.5 % completed\n",
      "LOSS: 0.5860121250152588 ACCURCY: 0.6796438708536044\n",
      "current epoch 255 ,  63.75 % completed\n",
      "LOSS: 0.6010521054267883 ACCURCY: 0.6799235488721804\n",
      "current epoch 256 ,  64.0 % completed\n",
      "LOSS: 0.5902994275093079 ACCURCY: 0.679212394515701\n",
      "current epoch 257 ,  64.25 % completed\n",
      "LOSS: 0.6013707518577576 ACCURCY: 0.6798212401592215\n",
      "current epoch 258 ,  64.5 % completed\n",
      "LOSS: 0.5956189036369324 ACCURCY: 0.6804557098628926\n",
      "current epoch 259 ,  64.75 % completed\n",
      "LOSS: 0.5969828367233276 ACCURCY: 0.6818005555064129\n",
      "current epoch 260 ,  65.0 % completed\n",
      "LOSS: 0.5848571062088013 ACCURCY: 0.6809879380804954\n",
      "current epoch 261 ,  65.25 % completed\n",
      "LOSS: 0.5877672433853149 ACCURCY: 0.68036270322866\n",
      "current epoch 262 ,  65.5 % completed\n",
      "LOSS: 0.5882906913757324 ACCURCY: 0.6804241662980982\n",
      "current epoch 263 ,  65.75 % completed\n",
      "LOSS: 0.597839891910553 ACCURCY: 0.6800350924369748\n",
      "current epoch 264 ,  66.0 % completed\n",
      "LOSS: 0.6010985970497131 ACCURCY: 0.6802847837240159\n",
      "current epoch 265 ,  66.25 % completed\n",
      "LOSS: 0.5888292789459229 ACCURCY: 0.6804056293675366\n",
      "current epoch 266 ,  66.5 % completed\n",
      "LOSS: 0.5891127586364746 ACCURCY: 0.6797542467934544\n",
      "current epoch 267 ,  66.75 % completed\n",
      "LOSS: 0.592518150806427 ACCURCY: 0.6805382467934542\n",
      "current epoch 268 ,  67.0 % completed\n",
      "LOSS: 0.5775235295295715 ACCURCY: 0.6810406227333039\n",
      "current epoch 269 ,  67.25 % completed\n",
      "LOSS: 0.5883059501647949 ACCURCY: 0.6809407837240158\n",
      "current epoch 270 ,  67.5 % completed\n",
      "LOSS: 0.5854308605194092 ACCURCY: 0.6800611729323307\n",
      "current epoch 271 ,  67.75 % completed\n",
      "LOSS: 0.601171612739563 ACCURCY: 0.6797580185758514\n",
      "current epoch 272 ,  68.0 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.6047000885009766 ACCURCY: 0.6805454011499336\n",
      "current epoch 273 ,  68.25 % completed\n",
      "LOSS: 0.5956793427467346 ACCURCY: 0.6804880990712076\n",
      "current epoch 274 ,  68.5 % completed\n",
      "LOSS: 0.6019219160079956 ACCURCY: 0.6799660185758515\n",
      "current epoch 275 ,  68.75 % completed\n",
      "LOSS: 0.6074947714805603 ACCURCY: 0.6807934011499337\n",
      "current epoch 276 ,  69.0 % completed\n",
      "LOSS: 0.5787862539291382 ACCURCY: 0.6805366227333038\n",
      "current epoch 277 ,  69.25 % completed\n",
      "LOSS: 0.5835028290748596 ACCURCY: 0.6801690119416187\n",
      "current epoch 278 ,  69.5 % completed\n",
      "LOSS: 0.5871630311012268 ACCURCY: 0.6809471596638655\n",
      "current epoch 279 ,  69.75 % completed\n",
      "LOSS: 0.587467610836029 ACCURCY: 0.6807744077841662\n",
      "current epoch 280 ,  70.0 % completed\n",
      "LOSS: 0.5836055874824524 ACCURCY: 0.6802393206545777\n",
      "current epoch 281 ,  70.25 % completed\n",
      "LOSS: 0.5830278992652893 ACCURCY: 0.6804006227333039\n",
      "current epoch 282 ,  70.5 % completed\n",
      "LOSS: 0.6138986349105835 ACCURCY: 0.6809318708536047\n",
      "current epoch 283 ,  70.75 % completed\n",
      "LOSS: 0.5903733372688293 ACCURCY: 0.6812657903582484\n",
      "current epoch 284 ,  71.0 % completed\n",
      "LOSS: 0.6033633351325989 ACCURCY: 0.6805790252100841\n",
      "current epoch 285 ,  71.25 % completed\n",
      "LOSS: 0.5929749608039856 ACCURCY: 0.6810111596638656\n",
      "current epoch 286 ,  71.5 % completed\n",
      "LOSS: 0.5903608798980713 ACCURCY: 0.6814376293675365\n",
      "current epoch 287 ,  71.75 % completed\n",
      "LOSS: 0.595632791519165 ACCURCY: 0.6806451729323308\n",
      "current epoch 288 ,  72.0 % completed\n",
      "LOSS: 0.5915059447288513 ACCURCY: 0.6825614011499337\n",
      "current epoch 289 ,  72.25 % completed\n",
      "LOSS: 0.5856326222419739 ACCURCY: 0.6793416293675365\n",
      "current epoch 290 ,  72.5 % completed\n",
      "LOSS: 0.597095251083374 ACCURCY: 0.6810426360017692\n",
      "current epoch 291 ,  72.75 % completed\n",
      "LOSS: 0.6003370881080627 ACCURCY: 0.6811185422379478\n",
      "current epoch 292 ,  73.0 % completed\n",
      "LOSS: 0.600553572177887 ACCURCY: 0.6812827032286599\n",
      "current epoch 293 ,  73.25 % completed\n",
      "LOSS: 0.5886276960372925 ACCURCY: 0.6809812401592217\n",
      "current epoch 294 ,  73.5 % completed\n",
      "LOSS: 0.6043308973312378 ACCURCY: 0.680478085802742\n",
      "current epoch 295 ,  73.75 % completed\n",
      "LOSS: 0.6018189191818237 ACCURCY: 0.6799617903582487\n",
      "current epoch 296 ,  74.0 % completed\n",
      "LOSS: 0.593571126461029 ACCURCY: 0.6814776293675365\n",
      "current epoch 297 ,  74.25 % completed\n",
      "LOSS: 0.5730321407318115 ACCURCY: 0.68180270322866\n",
      "current epoch 298 ,  74.5 % completed\n",
      "LOSS: 0.5930554270744324 ACCURCY: 0.6808275488721806\n",
      "current epoch 299 ,  74.75 % completed\n",
      "LOSS: 0.5891870856285095 ACCURCY: 0.6818877770897833\n",
      "current epoch 300 ,  75.0 % completed\n",
      "LOSS: 0.5833596587181091 ACCURCY: 0.680678475011057\n",
      "current epoch 301 ,  75.25 % completed\n",
      "LOSS: 0.5970075130462646 ACCURCY: 0.6810678708536044\n",
      "current epoch 302 ,  75.5 % completed\n",
      "LOSS: 0.6028584241867065 ACCURCY: 0.6813201662980981\n",
      "current epoch 303 ,  75.75 % completed\n",
      "LOSS: 0.6133154034614563 ACCURCY: 0.6810657903582485\n",
      "current epoch 304 ,  76.0 % completed\n",
      "LOSS: 0.6024171710014343 ACCURCY: 0.680857333923043\n",
      "current epoch 305 ,  76.25 % completed\n",
      "LOSS: 0.5893743634223938 ACCURCY: 0.6806751596638655\n",
      "current epoch 306 ,  76.5 % completed\n",
      "LOSS: 0.6031029224395752 ACCURCY: 0.6819154816452895\n",
      "current epoch 307 ,  76.75 % completed\n",
      "LOSS: 0.5815252661705017 ACCURCY: 0.6803015621406457\n",
      "current epoch 308 ,  77.0 % completed\n",
      "LOSS: 0.6030079126358032 ACCURCY: 0.6803968509509067\n",
      "current epoch 309 ,  77.25 % completed\n",
      "LOSS: 0.5779015421867371 ACCURCY: 0.6822810119416188\n",
      "current epoch 310 ,  77.5 % completed\n",
      "LOSS: 0.5891062021255493 ACCURCY: 0.6810372401592217\n",
      "current epoch 311 ,  77.75 % completed\n",
      "LOSS: 0.5837013721466064 ACCURCY: 0.6808696293675365\n",
      "current epoch 312 ,  78.0 % completed\n",
      "LOSS: 0.6004526019096375 ACCURCY: 0.6794106360017692\n",
      "current epoch 313 ,  78.25 % completed\n",
      "LOSS: 0.5974430441856384 ACCURCY: 0.6805576965944273\n",
      "current epoch 314 ,  78.5 % completed\n",
      "LOSS: 0.5886960625648499 ACCURCY: 0.6825710924369749\n",
      "current epoch 315 ,  78.75 % completed\n",
      "LOSS: 0.5946700572967529 ACCURCY: 0.6808456293675365\n",
      "current epoch 316 ,  79.0 % completed\n",
      "LOSS: 0.5982785224914551 ACCURCY: 0.6815584750110569\n",
      "current epoch 317 ,  79.25 % completed\n",
      "LOSS: 0.593707263469696 ACCURCY: 0.6806674816452896\n",
      "current epoch 318 ,  79.5 % completed\n",
      "LOSS: 0.5889372825622559 ACCURCY: 0.6809614011499334\n",
      "current epoch 319 ,  79.75 % completed\n",
      "LOSS: 0.5985931754112244 ACCURCY: 0.6801394816452898\n",
      "current epoch 320 ,  80.0 % completed\n",
      "LOSS: 0.5959559679031372 ACCURCY: 0.6810590924369748\n",
      "current epoch 321 ,  80.25 % completed\n",
      "LOSS: 0.5958653092384338 ACCURCY: 0.6816612401592217\n",
      "current epoch 322 ,  80.5 % completed\n",
      "LOSS: 0.5913837552070618 ACCURCY: 0.6820755488721805\n",
      "current epoch 323 ,  80.75 % completed\n",
      "LOSS: 0.5966493487358093 ACCURCY: 0.6798359380804955\n",
      "current epoch 324 ,  81.0 % completed\n",
      "LOSS: 0.6042221188545227 ACCURCY: 0.6792114816452898\n",
      "current epoch 325 ,  81.25 % completed\n",
      "LOSS: 0.5876103043556213 ACCURCY: 0.6803273206545775\n",
      "current epoch 326 ,  81.5 % completed\n",
      "LOSS: 0.5926416516304016 ACCURCY: 0.6809525555064131\n",
      "current epoch 327 ,  81.75 % completed\n",
      "LOSS: 0.5809063911437988 ACCURCY: 0.6806136293675366\n",
      "current epoch 328 ,  82.0 % completed\n",
      "LOSS: 0.601264476776123 ACCURCY: 0.6817529447147279\n",
      "current epoch 329 ,  82.25 % completed\n",
      "LOSS: 0.5963703393936157 ACCURCY: 0.6808696293675367\n",
      "current epoch 330 ,  82.5 % completed\n",
      "LOSS: 0.5963382720947266 ACCURCY: 0.6807984077841663\n",
      "current epoch 331 ,  82.75 % completed\n",
      "LOSS: 0.5913389325141907 ACCURCY: 0.6807462467934543\n",
      "current epoch 332 ,  83.0 % completed\n",
      "LOSS: 0.5910985469818115 ACCURCY: 0.6825635488721805\n",
      "current epoch 333 ,  83.25 % completed\n",
      "LOSS: 0.5955004692077637 ACCURCY: 0.6798965555064131\n",
      "current epoch 334 ,  83.5 % completed\n",
      "LOSS: 0.5940784811973572 ACCURCY: 0.6820086227333038\n",
      "current epoch 335 ,  83.75 % completed\n",
      "LOSS: 0.587293267250061 ACCURCY: 0.6799403272888103\n",
      "current epoch 336 ,  84.0 % completed\n",
      "LOSS: 0.5931923985481262 ACCURCY: 0.6804439380804955\n",
      "current epoch 337 ,  84.25 % completed\n",
      "LOSS: 0.600389301776886 ACCURCY: 0.6816582467934542\n",
      "current epoch 338 ,  84.5 % completed\n",
      "LOSS: 0.6043230295181274 ACCURCY: 0.6806636426360017\n",
      "current epoch 339 ,  84.75 % completed\n",
      "LOSS: 0.5874912142753601 ACCURCY: 0.6801727837240159\n",
      "current epoch 340 ,  85.0 % completed\n",
      "LOSS: 0.6020128130912781 ACCURCY: 0.6796847164971251\n",
      "current epoch 341 ,  85.25 % completed\n",
      "LOSS: 0.5928273797035217 ACCURCY: 0.6805799380804956\n",
      "current epoch 342 ,  85.5 % completed\n",
      "LOSS: 0.587459921836853 ACCURCY: 0.681110475011057\n",
      "current epoch 343 ,  85.75 % completed\n",
      "LOSS: 0.5972668528556824 ACCURCY: 0.6818026360017692\n",
      "current epoch 344 ,  86.0 % completed\n",
      "LOSS: 0.5871932506561279 ACCURCY: 0.6811904750110571\n",
      "current epoch 345 ,  86.25 % completed\n",
      "LOSS: 0.5797667503356934 ACCURCY: 0.6818990924369747\n",
      "current epoch 346 ,  86.5 % completed\n",
      "LOSS: 0.600563108921051 ACCURCY: 0.6802102467934542\n",
      "current epoch 347 ,  86.75 % completed\n",
      "LOSS: 0.5944113731384277 ACCURCY: 0.6828237770897831\n",
      "current epoch 348 ,  87.0 % completed\n",
      "LOSS: 0.6069430708885193 ACCURCY: 0.6808616293675365\n",
      "current epoch 349 ,  87.25 % completed\n",
      "LOSS: 0.5974717736244202 ACCURCY: 0.6809525555064134\n",
      "current epoch 350 ,  87.5 % completed\n",
      "LOSS: 0.5909131169319153 ACCURCY: 0.6812090119416188\n",
      "current epoch 351 ,  87.75 % completed\n",
      "LOSS: 0.5937709808349609 ACCURCY: 0.6820587032286598\n",
      "current epoch 352 ,  88.0 % completed\n",
      "LOSS: 0.6021252870559692 ACCURCY: 0.682030475011057\n",
      "current epoch 353 ,  88.25 % completed\n",
      "LOSS: 0.5794640779495239 ACCURCY: 0.6822279380804954\n",
      "current epoch 354 ,  88.5 % completed\n",
      "LOSS: 0.6122512221336365 ACCURCY: 0.6810886227333037\n",
      "current epoch 355 ,  88.75 % completed\n",
      "LOSS: 0.6027998328208923 ACCURCY: 0.680190475011057\n",
      "current epoch 356 ,  89.0 % completed\n",
      "LOSS: 0.5987183451652527 ACCURCY: 0.6805976293675365\n",
      "current epoch 357 ,  89.25 % completed\n",
      "LOSS: 0.5654860138893127 ACCURCY: 0.6809757098628926\n",
      "current epoch 358 ,  89.5 % completed\n",
      "LOSS: 0.5818181037902832 ACCURCY: 0.6808030924369749\n",
      "current epoch 359 ,  89.75 % completed\n",
      "LOSS: 0.5939645767211914 ACCURCY: 0.6798683272888103\n",
      "current epoch 360 ,  90.0 % completed\n",
      "LOSS: 0.5937705636024475 ACCURCY: 0.681460005307386\n",
      "current epoch 361 ,  90.25 % completed\n",
      "LOSS: 0.6056596040725708 ACCURCY: 0.6801230924369748\n",
      "current epoch 362 ,  90.5 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.579623818397522 ACCURCY: 0.68297070322866\n",
      "current epoch 363 ,  90.75 % completed\n",
      "LOSS: 0.5863930583000183 ACCURCY: 0.6810153206545776\n",
      "current epoch 364 ,  91.0 % completed\n",
      "LOSS: 0.5664780735969543 ACCURCY: 0.6807984750110571\n",
      "current epoch 365 ,  91.25 % completed\n",
      "LOSS: 0.602611780166626 ACCURCY: 0.6826620858027422\n",
      "current epoch 366 ,  91.5 % completed\n",
      "LOSS: 0.5922765135765076 ACCURCY: 0.6805690119416188\n",
      "current epoch 367 ,  91.75 % completed\n",
      "LOSS: 0.5931670069694519 ACCURCY: 0.682266314020345\n",
      "current epoch 368 ,  92.0 % completed\n",
      "LOSS: 0.6014897227287292 ACCURCY: 0.6797273206545776\n",
      "current epoch 369 ,  92.25 % completed\n",
      "LOSS: 0.5954806804656982 ACCURCY: 0.6810410791685095\n",
      "current epoch 370 ,  92.5 % completed\n",
      "LOSS: 0.5980209112167358 ACCURCY: 0.6802387969924812\n",
      "current epoch 371 ,  92.75 % completed\n",
      "LOSS: 0.5904861688613892 ACCURCY: 0.6814283945157008\n",
      "current epoch 372 ,  93.0 % completed\n",
      "LOSS: 0.5835344791412354 ACCURCY: 0.6813934011499337\n",
      "current epoch 373 ,  93.25 % completed\n",
      "LOSS: 0.5928911566734314 ACCURCY: 0.6823285555064131\n",
      "current epoch 374 ,  93.5 % completed\n",
      "LOSS: 0.6044511198997498 ACCURCY: 0.6814679380804953\n",
      "current epoch 375 ,  93.75 % completed\n",
      "LOSS: 0.6107262372970581 ACCURCY: 0.6822257903582484\n",
      "current epoch 376 ,  94.0 % completed\n",
      "LOSS: 0.5927388072013855 ACCURCY: 0.6819534683768244\n",
      "current epoch 377 ,  94.25 % completed\n",
      "LOSS: 0.6024049520492554 ACCURCY: 0.6804629314462628\n",
      "current epoch 378 ,  94.5 % completed\n",
      "LOSS: 0.5874507427215576 ACCURCY: 0.6809757098628926\n",
      "current epoch 379 ,  94.75 % completed\n",
      "LOSS: 0.5916228890419006 ACCURCY: 0.6811572401592216\n",
      "current epoch 380 ,  95.0 % completed\n",
      "LOSS: 0.596279501914978 ACCURCY: 0.6808207164971252\n",
      "current epoch 381 ,  95.25 % completed\n",
      "LOSS: 0.5982317924499512 ACCURCY: 0.6811605555064132\n",
      "current epoch 382 ,  95.5 % completed\n",
      "LOSS: 0.5839603543281555 ACCURCY: 0.6810747032286599\n",
      "current epoch 383 ,  95.75 % completed\n",
      "LOSS: 0.5887428522109985 ACCURCY: 0.6806250119416188\n",
      "current epoch 384 ,  96.0 % completed\n",
      "LOSS: 0.5869528651237488 ACCURCY: 0.6804624750110569\n",
      "current epoch 385 ,  96.25 % completed\n",
      "LOSS: 0.6137527823448181 ACCURCY: 0.6827411729323307\n",
      "current epoch 386 ,  96.5 % completed\n",
      "LOSS: 0.582355797290802 ACCURCY: 0.6824637098628926\n",
      "current epoch 387 ,  96.75 % completed\n",
      "LOSS: 0.6077716946601868 ACCURCY: 0.6815993206545776\n",
      "current epoch 388 ,  97.0 % completed\n",
      "LOSS: 0.5964512825012207 ACCURCY: 0.6820502467934543\n",
      "current epoch 389 ,  97.25 % completed\n",
      "LOSS: 0.5991313457489014 ACCURCY: 0.6815479380804952\n",
      "current epoch 390 ,  97.5 % completed\n",
      "LOSS: 0.5860241055488586 ACCURCY: 0.6807997770897833\n",
      "current epoch 391 ,  97.75 % completed\n",
      "LOSS: 0.6158835887908936 ACCURCY: 0.6813837770897831\n",
      "current epoch 392 ,  98.0 % completed\n",
      "LOSS: 0.6080740690231323 ACCURCY: 0.6805563272888102\n",
      "current epoch 393 ,  98.25 % completed\n",
      "LOSS: 0.5957762598991394 ACCURCY: 0.6802119380804954\n",
      "current epoch 394 ,  98.5 % completed\n",
      "LOSS: 0.5968279242515564 ACCURCY: 0.6805357098628926\n",
      "current epoch 395 ,  98.75 % completed\n",
      "LOSS: 0.595502495765686 ACCURCY: 0.6810216293675366\n",
      "current epoch 396 ,  99.0 % completed\n",
      "LOSS: 0.5913734436035156 ACCURCY: 0.6811782467934543\n",
      "current epoch 397 ,  99.25 % completed\n",
      "LOSS: 0.5948269963264465 ACCURCY: 0.6810052401592216\n",
      "current epoch 398 ,  99.5 % completed\n",
      "LOSS: 0.5895527005195618 ACCURCY: 0.6807327837240159\n",
      "current epoch 399 ,  99.75 % completed\n",
      "LOSS: 0.5987752079963684 ACCURCY: 0.6819782467934542\n",
      "current epoch 400 ,  100.0 % completed\n"
     ]
    }
   ],
   "source": [
    "model_trainingInfo = trainMyModel(modelCarlos, criterion, train_loader, validation_loader, optimizer, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb285af24e0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2dd5gURfrHvy/LEkSEJeekeAooICuY5U5EMGEWTNypp3hyvzOf6JnOcJyeekYQlTOciijmACJIFmXJOWdYWBBuCcsusO/vj7fLrunpme3ZnbC7836eZ55O1d3VPTP1rXrrrbeImaEoiqKkH1VSnQFFURQlNagAKIqipCkqAIqiKGmKCoCiKEqaogKgKIqSplRNdQZioUGDBtymTZtUZ0NRFKVCMXv27B3M3NC7v0IJQJs2bZCTk5PqbCiKolQoiGi93341ASmKoqQpKgCKoihpigqAoihKmqICoCiKkqaoACiKoqQpKgCKoihpigqAoihKmpIeAvD118DQoanOhaIoSrkiPQTgu++Af/wj1blQFEUpV6SHADRsCOTnA4WFqc6JoihKuSE9BKBBA1nu2JHafCiKopQj0kMAGjoxkPLyUpsPRVGUckQgASCiPkS0nIhWEdH9PsfvJaJ5zmcRER0monrRziWiekQ0nohWOsus+D2WByMA2gJQFEX5lRIFgIgyALwCoC+ADgAGEFEHOw0zP8PMXZi5C4AhACYz8y8lnHs/gAnM3B7ABGc7MWgLQFEUJYwgLYDuAFYx8xpmLgIwCkC/KOkHAPggwLn9ALztrL8N4JJYMx8YFQBFUZQwgghAcwAbre1Nzr4wiOgIAH0AjAlwbmNm3goAzrJRhGveQkQ5RJSTV9oCPCsLqFIlVADGj1eTkKIoaU0QASCffRwh7UUApjPzL6U41xdmHsHM2cyc3bBh2IQ2wcjIAOrVcwXgpZeA3r2Bp58u3fUURVEqAUEEYBOAltZ2CwBbIqTtD9f8U9K524ioKQA4y+1BMlxqGjUCtju3eOstWapJSFGUNCaIAMwC0J6I2hJRNUgh/4U3ERHVAXA2gM8DnvsFgIHO+kDPefGnVStgvTMr2i9OA2V7YjVHURSlPFOiADDzIQCDAYwDsBTAaGZeTESDiGiQlfRSAN8x876SznUODwVwLhGtBHCus5042rUD1qyRdSMAubkJvaWiKEp5JtCk8Mz8DYBvPPuGe7bfAvBWkHOd/TsBnBM8q2WkbVtg927p+M3Pl31btybt9oqiKOWN9BgJDIgAAMCcObKsUwfYtg04fDh1eVIURUkh6SMA7drJ8s03ZdmxI1BcLC2CF14AZs1KXd4URVFSQPoIwNFHy3L0aFl27CjLrVuBO+4AundPTb4URVFSRPoIwFFHAePGudtGAFaudPepW6iiKGlE+ggAAPTq5a4bAVi2zN03fnxy86MoipJC0ksAqliP28GJSbd0qbsvJye5+VEURUkh6SUANk2aiCeQ3QJQAVAUJY0INA6gUrF4MfDjj9IaaNrUbQGccoq4iB4+LLGDFEVRKjnp1wLo0AG46SZZb9IEOHBA1i+4ANi3T8RBURQlDUg/AbBp2tRdv+YaWZ55JjB4cGryoyiKkkTSWwCaNHHXW7YELnHmpHnlFeC771KTJ0VRUgIHDFR/+HDliSKT3gJgBocBQGYm8O67Eh6iXj3gv/9NXb4UpQJz333AmDHR0xw6FL5vnxNGctw4YOFCWS8qAq6+GpgxIzz9ihXuOX7HLrtMAgDv2RM9LwcPAscdB/zpT7Kdmyse4e++65/Pxx8HmjUD5s0DTj0VeO89YOJE93hxMfDaa8CUKcD77wN794Zfgxk491w5f/p0YN26yPljlmsFFaiYYOYK8+nWrRvHlfnzmeW9hu7//e+Z69RhXrNGtt96i/m3v2UuLo7v/RUlRnbsiP4z/Pxz5v/8x93evZv54MHwdLt2MXfqxPzYY/7XWb2aeefOyPcpKJBlcTHzhAnMU6cyT5zIfOWV7l/qxx+ZX3uNedw45sOHmYcPZ27cmLlGDeZmzZi//575+efl3Bkz5Jw33nDP37GD+bPP3O3bb5d9+/cz/+EP7j5m5n375NlPPJF5yRLmM85wz+vbl3nMGObLLmPeu1fSr1zJfPzxbhrz+e475rp13e2nn5b0eXnyrP/4h3ssMzP03I0bmXNymM86K3T/mWcyL1vGPGUK87RpzJMmMc+bF5qmalXme+5hHjpU8lhcLNcaO5Z51ChJ8+GHkb+PkgCQwz5lasoL9Vg+cReAw4f9BeDHH5lr1mRu0IB52zY3zcKF8b2/krYUF0cvyHfvlkKtuFgKyv37mRcsYM7IkJ9i48bMXbtKveTNN5nz80N/zlu3Mm/ZwtyoEfPvficF4D33iBgUFzP/5S9u2ksukYL4pZeYn3uO+cYbZX+NGlKo3nYb88CBIhinncZ84YVy/OyzmXv3Di9E/T49ezJXqxYsrfnUrs3csWP0NPXqiYDUr+/uI4qc/uKLmT/+WOp3kdI0bsw8erQU3NWrM59zTmz5BphPPVW+m+uvj55u7Fipb5p3Csj3df75oek6dGAuKir97y2SAJAcqxhkZ2dzTrx99fv1E5fQTz8N3b9wIZCdLf0CkybJ5DH/+Adw//3xvb9SLmAGli8XUwAArF4N/PADcN55Mplc9erh52zcCDz2mPxEevcGpk0D3ngD6NkTOPlkMV906ybWxLPOkiEnK1cC33wjpon164FHHwXGjgVq1ZKup48+Es/kd94BqlUDTjgB+Omn8HtnZQG7doXu69QJWLTI3W7RQmzV3oC3J58ssQ9vvhmoWVPMFUVFoWluvll+9qtWhe4302r06iVmGSLgr3+VZ+vQQe755pvA1KmSfs0aeb577xWHu7lz5XmY5S91zTWyPWmS5OE//5H3edtt8u4BObdDB5nG4513gPnzgX/9S+Z4uuoqSVOvHnD66cDvfy/Hd+8GXnwxNO+33ip5O3RIAgF8+SXQoAFw4YXAkiXA0KFAjRriEFi3rsSJ7NFD3vOAAcCrr7rXGjZMzE+NGwOjRgFff+0emzFDTDsAUFgoRUznzvIMRK6JqFMn4PXXJd3Bg3LN3FzJd9WqwAMPAF26iHnp6quBrl3DfwdBIaLZzJwddsBPFcrrJ+4tgJJ44olQGb7gguTeX4kbxcVSCx061P/4yJHyFb/3HvOXX4bWKAcMkDQ//sj8739LDe2KK5hPOaXkmqBtTojl06mT1K4bNWJu3565VSvmSy+Vmun27VIbvOMOMSecfDLzMcdIgxVgzspyrzNsGPPLL0tt/cYbmY87Tvb/9a/SYjDv5qefmFesYP7oIzHVMDPn5jI/8oi8jzVrmCdPlnPWr5fj+/f710pzc6UW/t577r61a8X8URL29Z55Rkw5+/eHptmwQZaFhcznnSemmJyc8GutWcP8zTdigjLnzJnDfNNNkh9DYaFrGvKSn+8emzmTefp0eU/e1tuwYczffuuaxsrCvn3hz1xWoC2AUnDwoFTDDKecouMEKhg//CC1wfx8qR02awZs2ADcc48cq1pVap7bt0v/v+HEE4E//AF44glg506ZTmLtWjmWlSXXZAYeeURaAYZ775V7zJkDfPihXH/wYODpp+X45ZdLbXX9eulkHDECuPRSGYT+xBPAyy9LzbZ1a+CII4I9Y1GR1Cz/9z+psffoIR2fU6cC558vxwzMUsu0PaATAXPofRNFcbG0DBo0SPy9KjKRWgCBBICI+gB4AUAGgDeYOWz6RiLqCeDfADIB7GDms4noNwA+tJK1A/AwM/+biB4F8EcAJgTnAyyzh0Uk6QIASFtxyRLxEmrXLjR0hJIUjLmgbl0xxSxbJs3y7GwxeXTpIp4jQ51f5dlnS7oXXog+7XP79mIOMAX7c88B338PtGkDPPusmAMKCqRgXrdOCvaOHYGLL5ZzqlaVpv3XX4sppWtXEQdDUZHUIWrVEtHZtk3ML4qSbEotAESUAWAFZN7eTZCJ3gcw8xIrTV0AMwD0YeYNRNSImbf7XGczgB7MvN4RgL3M/K+gD5ESARg/XqpRffuKsdKuJiqlZvZsqbU1by41RW/0jf37gWuvlRpejx7Agw/K/rp1pfYNSGG8eLHUmDdskK6c4uLwe02aBCxYIKGffvpJbMTVq4toVK0qNtwjjxRbsKJURiIJQJBYQN0BrGLmNc6FRgHoB2CJleYaAJ8w8wYA8Bb+DucAWM3M62PNfEo591ypxt1/v/RmJattW4k5cEBq74AUwA0bArffDtxwA/D558DPP0tBbaZq+OIL4KSTxFSybZvU/keNEpPHgw+KH3afPmJymTXLjfo9ZYrU8M8+Wz6A3MNL//6Jf2ZFKY8EEYDmADZa25sA9PCkORZAJhFNAlAbwAvM/I4nTX8AH3j2DSaiGwDkALibmT1+DQAR3QLgFgBo1apVgOwmiKwsEYIdO+Rz/PGpy0s5p6hIvGjMKyosBK6/Xmr5L74Y6p1x9dXAJ58Af/ubfACxkRcWAm+9Jde6/XZxwOrd2z3vpZfERFOzpphoDD17Sk3+5pslqoeiKJEJIgB+1V2v3agqgG6QWn5NAD8S0UxmXgEARFQNwMUAhljnDAPwuHOtxwE8C+DGsBsxjwAwAhATUID8JgZj3O3SBdiyRUqmzMyUZac8wSzui6edJrXyXr2ko/G22+Q1bdzoRtYYNUqWRNISqFZN7OnLlgHffivmHhOWyTS0rrtOCnsb29Zuk5Eh7n2KopRMEAHYBKCltd0CwBafNDuYeR+AfUQ0BUBnSN8BAPQFMIeZfzWg2+tE9DqAr2LPfhIxJc4W59E3bnQnmk8znnxS/LnffFPs6rNnS03e0LixzMA5bJi7r0cPKehbtpQpmBs3dh2s2raVT9++/vfzFv6KosSHIAIwC0B7ImoL6cTtD7H523wO4GUiqgqgGsRE9Lx1fAA85h8iasrMJqTSpQAWoTzjrXKuWeOOijlwwJ1hrJJRWCh29Fq1ZHvZMtdUs2GDDHQyBXTr1pJ+4kQx40yfDmzaBPzlL2LGufhioHbt0InZFEVJHSUKADMfIqLBAMZB3EBHMvNiIhrkHB/OzEuJaCyABQCKIa6iiwCAiI6AeBDd6rn000TUBWICWudzvHzhJwCAG1CuAo2nCMqBA+ISuW0bcOedUoMfP148cXr1Aj7+WDpdAbG3T54cOnTissvktZx+unTiat+5opQvdCBYUNascQv7zEypyv70kziTA5VCAPLzxXQDSGSMhx4SN8uaNcUfnkiG5z/1lNT6O3QQD54WLWRou/q4K0r5pCxuoArgtgAaNZLRRb/8Ajz8sHu8ArqH7twJfPYZcOON4olzxRUyAKpnT6m9G3JzxY++Rg3XfRMQwahdO+nZVhQlTqg1NihZWRKBasYM6cUExOBtMKOTyjErV4qXjuFPfxJ3yRNOkMIfAO6+W+z6xk5frZq0Cs44I7TwB7TwV5SKjrYAYuHuu2X5/PMSHmKjNTwiLy+yb2I5YOtW4De/kQJ98WKZpGLyZDm2eLEsb7pJ0tx3n7QAnnxSPV0VpTKjAlBaWrUKjdOblwcce2zq8lMCL7wgVqrCQrHjr14devzWWyWsb5s24tx0zjnS2asoSuVFBaC0tGoVak/Jy4ucNsUMGwb8858S07xZM7HzDxwotv277xaXzW7d3PSXX566vCqKkjxUAEqLNyxFORGAIUOkBj9woMTVmTBBPHQuuggYOVJGyt50k0ayUBRFBaD0lEMBKCpyQyI/+KCbpe7dxWff+Odr4a8oCqBeQKXnhBNCt2fMAO66yz8ecZKYP99dz8uTqQffe09aAva8NoqiKIC2AEqPPQVRZqbMCvL118CgQUntDGaW8EQDB7o1/ttuA/78Z63pK4oSHRWAeNCtGzBzpqzv3Zu0286cKfF1bOtThw6hk1criqJEQk1AZcGMBO7e3d33yy9Ju/3LL0vhf/31Epnz889l9itFUZQgaAugLDz6qIjA66+7+3buTPht588Hli8HRo+WGa7efjvht1QUpRKiAlAWzGS2dkyEBLcADh4ELrgA2LxZtq+6KqG3UxSlEqMmoHhw4YXAWWfJ+j33SGdwAigslM7dzZslvv7MmSIGiqIopUFbAPGgTh0JrFOzJrB/vwhCnMND798vMehefx344x9lXl2dWEVRlLKgAhBPDhxIyGW3b5cpFdetAy69FBgxIiG3URQlzVABqAA88YT4+j/9tMTuVxRFiQeBjAhE1IeIlhPRKiK6P0KankQ0j4gWE9Fka/86IlroHMux9tcjovFEtNJZlt9YyrESbRbzVatCZ0uPwl13SeH/ySdi67/3XqB+/TjlUVGUtKfEKSGJKAPACsi8vpsgk8QPYOYlVpq6AGYA6MPMG4ioETNvd46tA5DNzDs8130awC/MPNQRlSxm/mu0vKR0SsggmBnBMjKkI3jePImxbNOokTjvFxZGjc9QVARUr+5uv/WWjPZVFEWJlUhTQgZpAXQHsIqZ1zBzEYBRAPp50lwD4BNm3gAApvAvgX4AjAf72wAuCXBO+Wb5cuDKK4HDh4E+fYD7fRpLZthuCf0Fc+e66z17aohmRVHiTxABaA7AmvoKm5x9NscCyCKiSUQ0m4husI4xgO+c/bdY+xsz81YAcJaN/G5ORLcQUQ4R5eSVg4ibUTn2WKCfRxv37PFPW1AQ9VLTp8ty1Spg4kTgyCPjkD9FURSLIJ3AfjOde+1GVQF0A3AOgJoAfiSimcy8AsDpzLyFiBoBGE9Ey5h5StAMMvMIACMAMQEFPS9lNGwYur11q//kuVEEYNYs4KmnJODo0UfHOX+KoigOQVoAmwC0tLZbANjik2YsM+9zbP1TAHQGAGbe4iy3A/gUYlICgG1E1BQAnGUQs1H5x08A/IhgApo+XUIL7dwpc/IqiqIkiiACMAtAeyJqS0TVAPQH8IUnzecAziSiqkR0BIAeAJYSUS0iqg0ARFQLQG8Ai5xzvgBgujUHOteo+LRtK/aaa66R7dxc/3QRWgBDhgBHHAF8+63M4qUoipIoShQAZj4EYDCAcQCWAhjNzIuJaBARDXLSLAUwFsACAD8DeIOZFwFoDGAaEc139n/NzGOdSw8FcC4RrYR4GA2N76OliLp1Za7gl16S7RkzJICPFx8BWL8emDoVeOgh6UNWFEVJJIEGgjHzNwC+8ewb7tl+BsAznn1r4JiCfK65E9JnUPmoUgXIcoY1vPgi0Lq1OPXbeATg0CE3uvQVVyQhj4qipD0aTSZRkNV3bkJ32nj6AIYOBd55RwZ8HXNMgvOmKIoCFYDE8uabsjRiYJuCrBbA/v3ACy8AvXsDX36ZxPwpipLWqAAkkhtvBJo0AfLzZdssgRAB+PxzYMcOGTRMfk63iqIoCUAFINHUqSOdwkBEAXjvPaBFCxnxqyiKkiw0GmiiOeooKfj/+U8Z4WVw+gB27ADGjZM+4rjG99+zRy7etm0cL6ooSmVCBSDR1KkDrFkDjB0but9pAYweLR5A114b5/v27AnMmRP3iWkURak8qAko0Rx1FLBihawPHixTRgK/CsBHHwEdOwInnug5r6DADRxXGubMKf25iqKkBSoAiaZOHVk2ayaDw555RsJAFxRg505g2rQII3579ZLQ0YcOAR9+WPqavPc8P5dURVHSEhWARHPUUbJs187dV7Mmlm6pgwYNpHw//3wAxcViszfMmCHLF18E+vcH3n+/dPc/dMhdnzRJeps/+qh011IUpVKhApBoTAugVSt3X82aGLO0AwAp/E89FdI6aNgQWLs29PztToy8NWtKd//CQnfdTKbz00+lu5aiKJUKFYBEU7OmLO0ooTVr4pv1HXHyyTJxWNWqkKD/QKinEOAcROR5BUqiqMhdN62Bqtr3ryiKegElHuP7X6/er7v2ZNbDTzva4YFBkD6B5cuBlk7E7XXrQs834wXsMQSxYLcAjABkZpbuWoqiVCpUABKNCf/QpMmvu2ZxNoqRgdNP3ANcdZ/sPOMMWS5ZEnq+CSe9axcwe7YECjJmpX37pMUQbQSZnwBoC0BRFKgJKPEMGQLceSdwgztL5swDXQAAPa6y+gWmTZPl6NHAokXufjOhzObNIhIvvugeu+EG4Le/BbZ45+exUBOQoigR0JIg0dSrBzz3XMiuyQdPxXFYiizsDk9fUAD8/e/utmkBrF0ro4c3bXKPzZwpS7/5BgzaAlAUJQLaAkgyq1YB47ediKvuaS3jAQC3f+CMM4BzzgE2bHBPMC0AU8u3B4ft3y9L29XTi90CiCYUiqKkHSoASWbECCAjgzDoriOAH36Q2V/OPlsO1qkjncF2LX+3p5VgC4DpILZr+V7sY0YAoqVXFCVtCCQARNSHiJYT0Soiuj9Cmp5ENI+IFhPRZGdfSyL6gYiWOvv/YqV/lIg2O+fMI6Lz4/NI5ZfiYuCDD2S6x6ZNAZx2mgzKatpUEhgBiDSRPBAqAKYgDyoAZt1uFfgxaZLEpV6+PHo6RVEqNCUKABFlAHgFQF8AHQAMIKIOnjR1AbwK4GJm7gjgSufQIQB3M/PxAE4BcLvn3OeZuYvzCZlysjIyerRU7gcM8Bxo1EiWtWqJABQXR76IEQC7XyGaANiF/b59oelff10K+p07Q8/54ANZ/vBD5OsqilLhCdIC6A5gFTOvYeYiAKMA9POkuQbAJ8y8AQCYebuz3MrMc5z1PZBJ5ZvHK/MVicJCYNAgoEcPnzl/zSCxQ4fc8QCR+OUXudjdd7v7zj8f+POfI9/Y4BWA4c60zqtXh56TkeHmR1GUSksQAWgOYKO1vQnhhfixALKIaBIRzSaiGzzHQURtAHQFYMchGExEC4hoJBFl+d2ciG4hohwiyskrS3TMFDN/vswLc++9bt/vrxx5pCwLC/0FwIwmNixeHLq9axfw8stujf3wYfeYXwvA7DMDwrwFvREA+zqKolQ6ggiA3ySF3tCUVQF0A3ABgPMAPEREx/56AaIjAYwBcAczmyGtwwAcDaALgK0AnvW7OTOPYOZsZs5uaIdTqGCYMDzZ2T4Hq1eXZWEh0Lp1+PEGDUK3I4V6NjV5P7s/EN4CMO6g3j4Bs18FQFEqNUEEYBMAu1raAoB35NEmAGOZeR8z7wAwBUBnACCiTEjh/x4zf2JOYOZtzHyYmYsBvA4xNVVacnLE0mPHhPuVTp1kec45bmvAxit88+b538SZZezXJRBMAPbuDb2OtgAUJS0IIgCzALQnorZEVA1AfwBfeNJ8DuBMIqpKREcA6AFgKRERgDcBLGXmkNFQRNTU2rwUwCJUUpjFOnPKKREmfe/YUTx/Bg2Sbe/ckCefHLo9d67/jfwEwK7dm3EDRgCMCcgbaM4Ig/YBKEqlpkQBYOZDAAYDGAfpxB3NzIuJaBARDXLSLAUwFsACAD8DeIOZFwE4HcD1AH7n4+75NBEtJKIFAH4L4M54P1x5YeZMifEW1vlr06SJqw5Nm4Ye69MndLusLQAjCpEijZp82NdRFKXSESgmgOOi+Y1n33DP9jMAnvHsmwb/PgQw8/Ux5bQC8/bbYua/5JKAJzRt6s7c1aABULdu6PH9+6Xw9tbQTYEdtA8gUgvAHPeahhRFqVToSOAEs2sX8O67wDXXuJODlcjFF8ty+nRg/Xq3k9jm9NPFJdTGFNx+JqA33xQ3JJPulluAL7+UbW+oaXO+EQxFUSolKgAJ5q67JGLDHXfEcNKDDwILFshI4SOOCBUAs3700eHCEM0E9NprwAknSEdEQYEMAjN4WwDm/GgtgNxcMRUZEVGUePHxx67bnJJQVAASyJYtwFtvyZitE0+M4cQqVaSwNtgFvQkc165dZAGwzT6mBZCbC3TrJh5FdqwhILIARGsBmJDVnkinilJmrrwy3PFBSQgqAAnElJEXXFDGC9Wo4a6bgG7t2rnumoZILQBmEYDGjWUUmjfWkFcATJC5aC0Ak6c9e8Qraf36YM+ipCdmbmulXKECkEDMgN2OHct4Ibumb2r07dqFpztwQPoP7rvP3VdYKB0RBw+Kp5Fff0JpWgBGJPbsAU46CWjTpsTHUNKUt9+Wysfs2SWnZe8YUyWRqAAkkEWLJM5bmQcw24W2CRzXtm1omipVpOD+8ktg4UJ3f1GRO6lMJAGI1AkcrQVgxCHIZPWHD4eHtY4Ha9cGG6z23HPSX1HaeZWVsvHdd7L0Tnfqh4YqTyoqAAniwAFgxow41P6B0EL722+Bf//bFQJDVpY70MtQv778oWwBCAtEhNK1AMyxIIXqo49K/owXUjxYt05aQfbsaZF4+WVZemNJrVkTebBbPPOa7pjott4Bjn6UB9fjefPkt5EGqAAkiMceA5YtA267LQ4XswXgmGOAv/wlPE1WFrBtm7tdtaqMIQjSAvDWzmMRgCCuop9+Kssffyw5bVDMOIlvvy05rTEr2MOwt2wRTyrbXGaYNk3GXnxT6SOUJwcjAN4+Kz/KgwB07Sq/jTRABSBBTJkirvpXXlly2hLxq7V7ycoCNlpBW+vXl45abwvATwC2bgV+/tntYA7SCRzLH9X0D0yZEvyckti1S5bR5k4wGAGwa/tmDMXYseHpJ0+W5dSppc+f4mLMdOVFAHbsAL76KvH3qQCoACSA4mIJ/9y1a5wu6BtAyENWVqg5pn59oHZtMWWsWSNB5urUCZ8QvkoVaSX06OHOVGNaAPv3R+6Ui1bzHzkSePZZue5f/wqMHy/7p00r+TmCYlo7QToNTRo/+7LfPMkmnZ9YJovp02Wq0HjaxMeOBX76qeR08aa8CcDAgcBFF0WfeS9NCBQKQomN1aulfIybAAQhyzOdQoMG0k+wcKEUcl26hE7zmJ0tg20aNXJbCGPGyOxgRgCYpRA3BeGDD0rN+YknIgtAfj5w002yvnEj8MIL7jFjtokHpREAOzCeeUa/PoDyIAAzZ0qLaeNGMfvFg759ZVlc7FYq8vLK5qWQmystpWhNXSMAQVpryRAA89tZtsyNu7V9u3gqvf9+4u9fjtAWQAIwlo4uXZJ401q1QrcbNBCTz5Yt0hw56STZf8QRsvzd72TZuHHoeQsWhI4jMB3Lhw4BTz0ls4g9+GBkAbA7lO3CH5Cmd2n54gvglSw65c0AACAASURBVFfcbfMnDtJZ69cCMGaukgSAGfjkk5LnUY4nRMA998h6bq7kJ57Tc1apIr+L776TCsC4caW/1vnnA1ddFf17MAIQ5B0mQwBMTPZFVgBiM5fG88+7+559NtzbLhZ27hQhL8eoAMSZgwelgty5c5IFwB4sBogJqGlTKZDt5siLL8of/7jjZNvrTbRypQiAaVGYgtIU3pmZwIgRrmeNl0huoUcdJa2DWEwaGze6A8yuuAIYPNjtSDYCkJsrhfTmzTLiLlpBFKsAEMnkO5dfXrre/LvvBvp5Z0+NkW3bgDvvFMH2zgQXC96W0sqV4qYGlM00t2CBLL3zStsYAQjy3SdDAEwlyBYA0yIyvwtARHjdumAtFz/OPBM49dRyPbZBBSDOLFwov5n77gvm9RY3vAJgWgAGIwBZWcC557ody7bpqHp14NVX5Y9q9ufnA//9r9QYAbHvn3mmrHtNE8yR/8B16sjSbgXs2QP85z/hf5CJE8XU1KqV24HcoYMs33tPlkYA9u+Xe/797+K1Yya09+YLiF0ACgvdWus77/g/VzSee05aLkE4cAB4+OFwAc3Nde32XjffWPCG9t6/3/2BlraAA9zCPdp0reWtBWDe8bhxwIYNofv87h9krIsfS5fKshyHVVcBiDPmO+/cOck3NgJg7NamBQBIYW8KUIPJ4HnnyfLkk8UcNH++bJvCYcgQ4PrrJZgcIAXy2WfLuneu4rVrw/9APXvK0tiIr7vONTnceKN8zD0BKZjOOceNiGowhZQxPdkdePa6X4d5SQJQWBj6JzXrBQWh6fLzJW+tWoVHYi0rr70GPP64NB9tcnPdd2rCd5cG4+o7eLAs9+wJNvNbfr68U+PKa5gwAZg0yd2OZt4z311JAvDDD3Hym7a46y6ZiNvGOEts2wYMHRq6z2/A4u7d4fGzYqG0ApIEVADizNKl8r9q3z5JN7z8cqBFC1cAevaUQuSqq9wWwAknhBcenTrJn/YPfxCXyilTQjtpjYnos89kaVoAjRq58xZ7/yxHHx3u63/BBfLHN5MhTJokE9yceKJEfQRCCw9zzZ9/Dr2O+YMWFIi9dvly4KyzZN/Onf6+/oZoArB7t/SfXHedpDt40DUjbdgArFjhnvPhh9I62bixbOYYP0zt3luT3rbNFYCy9EOY99rSmd3VLpSitQBWrpTlY4+F7u/VC/jtb93teAjAM8+EbpelZWJ4/nngX/8KbT3l58vvsmtX6QgG3PfhJwCjRsl7K+24EHPt116TfoUDB9xopzt3yr4UmYkCCQAR9SGi5US0iojuj5CmpzPj12IimlzSuURUj4jGE9FKZ5nld92KxtKlUg4Gcd2PCx9/LAWSuWHr1tJJ27Kl2wIwHcBe6teXArNuXRGQ0aOB228XIfDGrzYFoS0Afn96OwyFuUdmZvjE9nY6uwZvCl+ve6YtAO+8I/k2A+J27nQLi1tvlVq6H34CAEgNeMwYcQ/MynILgbfeCq2Rfv65u75unf89/CguBi67LNwF18YIl9fMY7cAggjAp5/K3A9ezLgJIwB797qtqWgmCnNPU0B9/HGo7dwQTQCimYCWLnWv7a01lcYF9qWXgN69ZZS4cT8GQsOW79kjfVLt27sCF62WbkJZ2NeLBfPbHTRI+hVuukla3Hl5wJ/+JPumTy/dtctIiQJARBkAXgHQF0AHAAOIqIMnTV0ArwK4mJk7ArgywLn3A5jAzO0BTHC2KzRLl8r3ePzxCbj4tGlup50fpuZoCmdACutLLgH69w92j8suk87dZs3CJ6dfsUJEpk4d9x4FBdJy6NXLTef909avL8toroZGAFasCJ2nwMDs/on27xcB6NXLNWN5OyAnTpQ/9B13SO3PbgF88IEUtn4F+LvvSqFo1/oNNWsCX3/tbsciAAUFUjBHM7UYAfB6V23bFj6TWzQuuwy4+ebw/UbUWrSQ5Z490c0eBiMczHLOlVeGhio32AKwa5e0zj78ULYjCcD48WKafPtt/3tHet78/MjzBfzf/8l1164FZs1yzZTG/dmcX7s2cOyxYtrZty+6ABgPIdMKDoLdt+S9thmEtnev+35TNPlSkBZAdwCrmHkNMxcBGAXA69pwDYBPmHkDADDz9gDn9gNgvvm3AQSdMLHccued8pu9664EXPz008WjIBLGW8YWgCpVpOAxLp+x4LXvAyIoRG4tEpAO4T/9yd22w1EArgB4xykAUtjXquX+sY4/PtQNz1BQ4BYi338vhe/vf+9e2zYBGV5+WdxQ773XHedQWOjOXzBnjpv24otD48/71WaNuckQiwDY/SJ2PhcskJrounVuGm9spdzc2LxoImEK+UaNRMj37g0mAKavwwhAJGzT1VdfydiA/v2lNWlaGF4BMK1A0wfkzUek5736avm+vK0l71zZubmhUWsN+fny3o89VrZXrYr+bOa/ZaKZfv+9/B7Md5aXJ9ew8To72Jj3vm+f2ypMppuxRRABaA7AijGATc4+m2MBZBHRJCKaTUQ3BDi3MTNvBQBn6fFHrFgUFclv/vrrw8uKpGBMFeeeG5/rGVc5G+MyavobTMvCTusdXWkK6YwMceW0zRMnnww0by7nMEe2+fq5dp5zjrRGMjLE3XLkyNDjfpEni4rcP5od7KtNG+lzsDujvdgeT8ceW7IA2AW9LQB2oTV8uBQOY8a4hZ/3/RnxMvkPes+rrgptyZjr160rrbugLQBbAKLVUu0Czw6zMG2aWwh7829qyaYQ3L1bWgSvvirbXtPU5MnSCjb9Q/a7yskJH3lpf8fmWQ8dkvzYArBiRbCO2tWr5dxHHpE/u/nNde8ebr6yK0KRrr13r/vsKQo+GEQA/OIQeHssqgLoBuACAOcBeIiIjg14bvSbE91CRDlElJMXzdUsxfz8s/y3S1PZjgu/+538Sb1+/aXFrwVgty6KilyXTFsAtmwJdUk1AgAAH30kXj8G00+xalV0n1m/yUSOOkpaI2aGNC85OTLa2X4fhYVurXLtWne/MU/Zz+fFHhDUpYuYmfwikfbpI1N52oWXLQBz5kjhvH+/a/bJz3cLYe9oabvQLKkFYHsmffSRTP9pMNevU0fMH++/7/ZpGDOE4auv3M5Rc83Dh10B+Ne/Qvt0GjQIbQGY2n/NmjIQygiAyf/cueLyavp5bAFo2NCdPNv7vHfeCTzwgPt7swXAjEewsWvlphA2y6OOckV9xYqSo9oad+TcXDftK6/If85UBmyxttfz8/3Fe88e1zkj3l5lAQkiAJsAWG1+tADgNYZtAjCWmfcx8w4AUwB0LuHcbUTUFACcpe+UQcw8gpmzmTm7YZkD6yeOceOkDDMekhUeu1A3BbpdCGZmuoW2nfbAAalhRjP9GLKyRABsc4wfxpe+bl1ZZmS4eYpUK162TOzdtkDYgfFszO/KjFUAwsdV2GavO++Updc1EpAfwo8/hhb6jzzirl93nRTOEye6roXLl7sFtN1C8HoSRGsBPPSQXNdm1y73urt3yzPVqCHfj13rt9cPHpQ4OccfL7UaUzDt3u0KwAknhAp5ixbuNYqLRbCPPlpaeD/+GN4CuOwycXk1Pvh2LbhOHdeV2QjA8OEijBs3Sn7M7822yftVDo3tHnALflN4164t5sfmzaUjuKQWwEUXybJ7d+kEb9lShMPuvLVNUHbe9uwJF1lAfiPGFdcWgE8/je/I7ygEEYBZANoTUVsiqgagPwDv6JbPAZxJRFWJ6AgAPQAsLeHcLwAMdNYHOteosHz1lZjpo5V3FQq7BWDW/WYh86YF5M81c6Z40fh5vhgbGZHbKRmNhx+WpQlbceSRbu05WtO5efPQL2Tz5vD0p53mH8emW7fQbXtQ3SmnyEi/JUv8g8kBoQJgew+ZQq+oyPVAWbrU3wxjiw4QuQVQVCSuv34+9HPnijli9GhXwGvXDk1j39s2nfXo4drpd+50BaBWrdDvvFkz973+8ou0Fho1kvc0d657/dWrRYDNuSZMwpYtIti7d4vI2wKwYYM81yWXiJlp1y437ElJAmB3xJqC/4EHZGlaGcce65qAosV+MiO6jalrxAhZmkGRgDyrwR43sHWr2Ia97N3rCr4RiPx8EcgkmRJKFABmPgRgMIBxkEJ9NDMvJqJBRDTISbMUwFgACwD8DOANZl4U6Vzn0kMBnEtEKwGc62xXSNatE/G/8MJU5ySO+P0ZIgmAtwP2yCOleT1woH/6iRPdwszEZQHC7aje0XTGnOP1UIqEtwVge4IAMtZh+vTQNG+8If7a3hHCxqXWzltRkWsqycsDbrnFPe7tDPeyerVbQ126NLS2avDGaSoslPs8/nho/qKFYXjtNekv2bpVvJzMdWxsl9BZs0KPGQE4eNB9Jq8ANG3qDpYyMYyMABQVuff7/nvpvDfCZsw2I0dKi8MIgGl9HTjgFrgm7S+/uC1PWwCizTlcp44bEsUEezO/O1sAolVGbM+nzz4TU5/X3dZ2j928WVqWtWqJqcjPhdTuiDctgCQHowsUDZSZvwHwjWffcM/2MwA8Izn8z3X27wQQwWG7YvHcc1LRDeptWSGwB1QZL5RIgbHatJFC2dR6SyqgMzLcpq8tADNnuqajUaPkpV5xhXvcbgEEoUkT12wEuIXsWWeJ+6qfyJlIpmbks30tGyNOY8ZI4XD77aEmGFO7j8R998k7fuMNqeH6hSb2CkBREXDGGVJg9e4tNXQgugAYV8y//c1t1fh5OS1YIF5mOTlSYI4bJwW4fW3TevETgH37xCxkCrpGjdzBhDY//RR5mrz//S+8BWAKSGM+2r/fre3bAhBtpG7z5nIdk/7vfxdTDiACsHOn/NaOOy5ciIcMEcGx+zzMc117rft7sfPw+OPy++nSRVo8fmZHQP4vxvRkBMC0imrWlIpVkFDwZUBHApcRZnFj7t8/tCyrVJx/viwjdZLWqiU/5E6dZDtoAQ2EvjTbXHP11eGmCtMCsPsc5s2T0cw2phbaqVNolFTTwWrGLUQbvOQ17Xif6bjjpBb42GPSh2EPNAJKFgBACo+bbhLx8MNrT1y/3vXssW3GJUVZff99mZbTYBfqJkS0MV9s2CD2e1vwzHdkTFleAbDdcQ2NGolpyNtvt3t3dG8rbx+AnzAaIbIFwOwz2OLevLn8Ps33f9pp7jHT6ty2LdzkBkgE3CeeCHVSMBWh6tVDfxdGAIzJskUL19Tkh58AmEmdCgr8+w3ijApAGcnLk8qF7UZe6Rg5Ugo0P88gG1Ngx9JZbwuAt7ZjF/SDBrl/ULtfoXNnt1PWcMcd8ofKzg53Z83MdPsgos1P4BUAIrnXkCGynZEhk90A0krxdtA+/njkaxuM/fhvf5Na9Omn+6fr3VuWdtTOtWtd+7ERgNNOA4YNc9NMmCD9MAMGhE7GYs5bskTMGfXquR2Yu3eL8NjfYf/+ErrDmIe8AmBaWXaha8T6ySfDnydawDe7BTBsWPRJW3bsEPG99trQFkCbNqHB+7wC0NzyYjeuoIA8o6FKlcjmGLtz3v59bd4cag49eDC8EmOoXt1fADZscMVm5kwJRZ5AdEKYMmJcjSOZxysFNWsGm5TE2KXtP1JJeN04n3/erTXZf65hwySUNRA+s5TXPFO3rlvz94pW8+bhgfH88Ovc9Q40Ml5Da9dKJ+HnMfoxGDNE/fpS8xs/3q2RA1KYFBSI4NWo4Q5EAqTVMHSoFBim5v3RR6F26EgdiU89JZ5JZsj6qafKCGkTTrtZs9B3f8wxcszY4b0CYN6D7UljWgV//KMUsrfeGt4H4+W440S0TQH71VfRRXrXLol95fXgueSSUNNh8+byHk0rwRYA26xp/27z8sJ/m6+/HhpCBHDdOH/zG3k+uxWUmSnv0v7eDFlZoX0Aa9bIuRs3yvcxfbq8c0Du6fVKixPaAigjxp28LPNGVBrMy4hlIgRT6zeF8h13uC6G3sLbbHsFoGFDKfhMbc4uvLwtgBYtwuMS+RFkZKbtNhrkmgbjK2zXPjMywoWMWf74Vau6teKOHd1CZ+NGqSGbFkD9+v4D+LwMGRL6fMOHixhccom0Cux+E0BMQnYneNWq/gJgD46zv6Ozzw7/zrzhJP7v/6Qz/MQTQ2vYtmeNzRFHiAD4uW/WqBFqsjHfzbJlYrKxa+X2vew8+RW4N98M/PnPoftMa9T8fi+7TJZt2sh7zc6W7R49QvsXjCvugQMyEv3QIelULiwMNVEBsYWgiBEVgDJiWgCVUgBq1w7mpmkwBVGssbB37/aP7eItzEyh4+da2rGjjBLNyQk1JXmv0by5HH///egToURy77SxC8qsLLfQWbnSjcczbJjUAAsKxN30wQelv2D9+vDBb14BMIUH4BZUJ5wQ7oL53/9KgVK9ejAB8NKiReg8Cl4BOOaY8Lz5CYAx7ZiWjY2pOZuWmVcAbFt6ixau372N3QfVpk3kkcmm8L7tNsmnaVEuWxZa+/dii1zQ6UCNGBsBmDpVlnffLfcy32FubugzHnmkuOYC0opp3tz1KurePVSA4jmVqgcVgDLALK3iJk1K978r9+zcGTqcviSMLd5bWJREnTr+/QuRBCDS5OL164f779sjkQHXlDVgQGSbOxCsI9vu4KtXT2y2994rNeYRI8SMMGiQRGM10VafeEKE1c9jwLa7L1oUGmPJnufBW/AtW+Y+Z2l/iLbHkVcAmjcPJgCA1ID9Jp43AmD6PaIJQLVqMvjP6wVlnxOtxmUKz1dflf4O8z39/LN8N16Mrd2uOASZwB5wBcAbhsK8EyMARx0V+oy22yqR5Mt08LdrFxrBd906iWuVgHhBKgBlYORI+U/bwTArFZmZsU1C8txz8YnhbvAWZuaPHfTPCUgH4eOPu3/uSKGxvXz7rTtZSCTslkhWlngCPP203IsoNrMQEPpcHTuGFkimBVCnjn9UUXMv79zQQbELJyMATz0lYwj8zFORBMArHgYjAFddJZ43ZiIiv/sbHn1UascPPSTbnTq5v0e7r8SbD6/5xq7ZX3dd+Hk9egCXXuqf75Iw+WndWsa3ePPSqJHU7D/7LPSdGbPO5ZfLO2nWzD3WsmWoR9INN4hpNOjscjGgAlAGvvxSKiJvvZXqnJQj4um37G0VGA+LWASgalXxsjHnemtqkTjmGNfLJ8i8vokeAm462P0K2MaNpUUDxKcpau4xZIgM3jL3sIlVAIznUXa21HS9/UR+AjBokIREMOLTurWYzgoK/FsAJo9e881pp4lrcNeuro0+Xpi8ZWSEVi7s1uGNN0qt3vw3rr3WLfA/+kjWjUjVqCFi3qdP6H2efDK0YztOqBdQKSkuFhPyRRfFVh4pMVClihQ0pgZoar7RJlYpCRPUKyh79gTzwIiXAGRm+vc/mBp0nTpi7rG9TeyBRmURgKpVRWj8CnGvO6MtANWqufmO9B7MMxmxIJJO7J49pXM0krsk4OanRQu3oPTLoyl0vd8XUfhsY5EYPjy0Jl8S77wjH6+g2aJoU1go79nM9GZEwTxXq1ayb+BAERTTn+Y3x0McUAEoJcbjyw4FoiQAOziaMS+VRgC+/loKnFhbKEEHtUWKShoreXn+ZjQjAHXrii172jQpJLyjbcviLlitWmQB8L43b+usQwcZ4BWpBWCwC8YWLdzvMtp77thRTFt2H4Cf0Jg8luUd3HqrfILSuHH4nMNAZAEwpjxvaBGzbUw/ROIR1a2b/G7jFeXXgwpAKTFTwgY1KStxoG9fCarlN7ioJMxo5kQRrxZAtJqjOd6unXwKCsJNBWUxwVWvLoIbKQ+PPOJ60XgF4NxzRQBKimvvrekb01Y0AejcOXzwmP2+n3xSQj+bP2WCfOZjItI7jIQxCXmdA378Mb79ah5UAEqJ6bAPMj5KiRM1aoSO8CxPJCsMrF3DjlRTfewxiRkUK8Z2HskcY4eTMAJgbOAPPCDjQPw6WQHxEHv++XDXV2PWiyV8CBD6vk2ETzP/QXkQgGghIPzwtgAMsThhlALtBC4lK1b4T52rpCmx1vgSeZ+HHy5dOGETviKIuaFKFRl/YOapzsqSCeMjxYt67rnwqLGAKwCxei8ZsfKbgKM8CECsBXebNtLSSfKEItoCKCUrV4YO5FTSlJdflo7DZHkClGRjLws33xxbZ+O115b9nkYASlNob9gQOs7DmL8SXGuOyogR0t8UKzVrhocaSQLaAigFq1eLuVEFQMHtt7sx85NBsloaycJMYlOaTvSWLUO9nowA+LU0ksUf/yg+/xUEFYBS8PvfS7/MNdekOidK2lEezBvx5MEHpSO4tAPYbMqDAFQwVABipLBQvPBuu60Szf+rVBwSPEFI0iGKn/nMdEgHjeOjBBMAIupDRMuJaBUR3e9zvCcR/Y+I5jmfh539v7H2zSOifCK6wzn2KBFtto4l2E8vPsydKyE5jMOBoiSFOXPc2b0Uf954Q6ZfrNSTc8SXEjuBiSgDwCuQeXs3AZhFRF8w8xJP0qnMHDIrLjMvB9DFus5mAJ9aSZ5n5n+VIf9Jx8zYZmbjU5Sk0LVr8DAW6UpWVmgAPaVEgrQAugNYxcxrmLkIwCgAAYKjhHEOgNXMvL4U55YbZs6UsRp27CZFUZSKSBABaA5go7W9ydnn5VQimk9E3xKR36zP/QF84Nk3mIgWENFIIkrSSJqyMXOmmn8URakcBBEAv14nbzf7HACtmbkzgJcAhPhBEVE1ABcD+MjaPQzA0RAT0VYAz/renOgWIsohopy8vLwA2U0cubkSjFAFQFGUykAQAdgEwB6f3AJAyBxlzJzPzHud9W8AZBKRHQy9L4A5zLzNOmcbMx9m5mIAr0NMTWEw8whmzmbm7IaxTDYeZw4fdt0+1ftHUZTKQBABmAWgPRG1dWry/QGEzExARE2IxD+NiLo717Xi1WIAPOYfIrLD4V0KYBHKMdOnS2jyZ5/VAHCKolQOSvQCYuZDRDQYwDgAGQBGMvNiIhrkHB8O4AoAtxHRIQAFAPozy2gMIjoC4kHkjVz1NBF1gZiT1vkcL1d8/LGMwbnlllTnRFEUJT4QV6BRc9nZ2ZzjN3l4EujYUeI1lSbMh6IoSiohotnMnO3dryOBA1BcLPF/OnRIdU4URVHihwpAADZvlhAQGvtfUZTKhApAAFavlqUKgKIolQkVgBLYv18mMwKAo49ObV4URVHiiQpACbz/vjtPg3e2NkVRlIqMCkAJLFggyzFjkjfpk6IoSjJQASiBhQsl9MNll6U6J4qiKPFFBSAKzNICOPHEVOdEURQl/qgARGH9euCXX4DOnVOdE0VRlPijAhCFCRNk2bNnSrOhKIqSEFQAIvDBB8DNNwN16wLHH5/q3CiKosQfFYAIDB8uy+uuq3zzcCuKogAqABFZtQq49lrghRdSnRNFUZTEoALgw+7dwJYtwAknAFX0DSmKUknR4s2HJUtk2dFvZmNFUZRKggqAD3PnyrJTp9TmQ1EUJZGoAPgwYQLQurV8FEVRKiuBBICI+hDRciJaRUT3+xzvSUT/I6J5zudh69g6Ilro7M+x9tcjovFEtNJZZsXnkcrG4cPAxIlAr17q/aMoSuWmRAEgogwArwDoC6ADgAFE5Dc31lRm7uJ8/u459ltnvz0l2f0AJjBzewATnO2UM3cu8L//Aeeck+qcKIqiJJYgLYDuAFYx8xpmLgIwCkC/ONy7H4C3nfW3AVwSh2uWmSlTZHn22anNh6IoSqIJIgDNAWy0tjc5+7ycSkTziehbIrL9ZxjAd0Q0m4husfY3ZuatAOAsG8WY94QwebLM/NWsWapzoiiKkliqBkjjZwlnz/YcAK2ZeS8RnQ/gMwDtnWOnM/MWImoEYDwRLWPmKUEz6IjGLQDQqlWroKeViuJiYOpUDf2sKEp6EKQFsAmAPRdWCwBb7ATMnM/Me531bwBkElEDZ3uLs9wO4FOISQkAthFRUwBwltv9bs7MI5g5m5mzGzZsGPjBSsOiRcCuXcBZZyX0NoqiKOWCIAIwC0B7ImpLRNUA9AfwhZ2AiJoQic8MEXV3rruTiGoRUW1nfy0AvQEsck77AsBAZ30ggM/L+jBlRe3/iqKkEyWagJj5EBENBjAOQAaAkcy8mIgGOceHA7gCwG1EdAhAAYD+zMxE1BjAp442VAXwPjOPdS49FMBoIroJwAYAV8b52WLiq6+Av/8daNtW/f8VRUkPiNlrzi+/ZGdnc05OTskJY2TFCgn53Lkz8OabQNeucb+FoihKyiCi2R43fADBOoErPePGSQfwJ58AbdqkOjeKoijJQUNBQFw/W7fWwl9RlPQi7QVg717ghx+041dRlPQj7QXgH/+Qid8HDUp1ThRFUZJLWgsAM/DWW0C/fsCpp6Y6N4qiKMklrQVg7lyZ+evSS1OdE0VRlOST1gIwbpws+/ZNbT4URVFSQVoLwPz54vnTqFyEoVMURUkuaS0AixbJxO+KoijpSNoKwNSpwOLFOu+voijpS1oKQG6uG/FTWwCKoqQraSkAX38ty9NO0w5gRVHSl7SMBfTllxL6Ydo0nfhdUZT0Je1aAMxi/+/VSwt/RVHSm7QTgBUrJPTDaaelOieKoiipJe0EYMYMWWroB0VR0p20E4DJk4F69YDf/CbVOVEURUktgQSAiPoQ0XIiWkVE9/sc70lE/yOiec7nYWd/SyL6gYiWEtFiIvqLdc6jRLTZOuf8+D2WP8zAd98B554LVEk76VMURQmlRC8gIsoA8AqAcwFsAjCLiL5g5iWepFOZ+ULPvkMA7mbmOc7k8LOJaLx17vPM/K8yPkNgZs0Ctm4FzjsvWXdUFEUpvwSpB3cHsIqZ1zBzEYBRAPoFuTgzb2XmOc76HgBLATQvbWbLwqFDwA03SNyfiy5KRQ4URVHKF0EEoDmAjdb2JvgX4qcS0Xwi+paIOnoPElEbAF0B/GTtHkxEC4hoJBFlBc927KxeDSxfDjz1FNCgQSLvpCiKUjEIIgB+3vLsbUn0DwAABh5JREFU2Z4DoDUzdwbwEoDPQi5AdCSAMQDuYOZ8Z/cwAEcD6AJgK4BnfW9OdAsR5RBRTl5eXoDs+rNsmSw19IOiKIoQRAA2AWhpbbcAsMVOwMz5zLzXWf8GQCYRNQAAIsqEFP7vMfMn1jnbmPkwMxcDeB1iagqDmUcwczYzZzds2DCGRwtl+XJZqvePoiiKEEQAZgFoT0RtiagagP4AvrATEFETIhlXS0TdnevudPa9CWApMz/nOaeptXkpgEWlf4ySWb4caNwYqFMnkXdRFEWpOJToBcTMh4hoMIBxADIAjGTmxUQ0yDk+HMAVAG4jokMACgD0Z2YmojMAXA9gIRHNcy75gNNKeJqIukDMSesA3BrnZwth+XKt/SuKotgQs9ecX37Jzs7mnJycUp3bsKHM/TtiRJwzpSiKUs4hotnMnO3dnxbDoXbuBHbs0BaAoiiKTVoIgHYAK4qihKMCoCiKkqakjQBkZgJt26Y6J4qiKOWHtBCA9u0lDETVtJz/TFEUxZ+0KBJvukk+iqIoiktatAAURVGUcFQAFEVR0hQVAEVRlDRFBUBRFCVNUQFQFEVJU1QAFEVR0hQVAEVRlDRFBUBRFCVNqVDhoIkoD8D6Up7eAMCOOGYnXmi+Yqe85k3zFRuar9goS75aM3PYlIoVSgDKAhHl+MXDTjWar9gpr3nTfMWG5is2EpEvNQEpiqKkKSoAiqIoaUo6CUB5nQxS8xU75TVvmq/Y0HzFRtzzlTZ9AIqiKEoo6dQCUBRFUSxUABRFUdKUtBAAIupDRMuJaBUR3Z/ivKwjooVENI+Icpx99YhoPBGtdJZZScjHSCLaTkSLrH0R80FEQ5z3t5yIzktyvh4los3OO5tHROenIF8tiegHIlpKRIuJ6C/O/pS+syj5Suk7I6IaRPQzEc138vWYsz/V7ytSvlL+G3PulUFEc4noK2c7se+LmSv1B0AGgNUA2gGoBmA+gA4pzM86AA08+54GcL+zfj+AfyYhH2cBOAnAopLyAaCD896qA2jrvM+MJObrUQD3+KRNZr6aAjjJWa8NYIVz/5S+syj5Suk7A0AAjnTWMwH8BOCUcvC+IuUr5b8x5353AXgfwFfOdkLfVzq0ALoDWMXMa5i5CMAoAP1SnCcv/QC87ay/DeCSRN+QmacA+CVgPvoBGMXMhcy8FsAqyHtNVr4ikcx8bWXmOc76HgBLATRHit9ZlHxFIln5Ymbe62xmOh9G6t9XpHxFImm/MSJqAeACAG947p+w95UOAtAcwEZrexOi/0ESDQP4johmE9Etzr7GzLwVkD80gEYpylukfJSHdziYiBY4JiLTDE5JvoioDYCukNpjuXlnnnwBKX5njjljHoDtAMYzc7l4XxHyBaT+N/ZvAPcBKLb2JfR9pYMAkM++VPq+ns7MJwHoC+B2IjorhXkJSqrf4TAARwPoAmArgGed/UnPFxEdCWAMgDuYOT9aUp99CcubT75S/s6Y+TAzdwHQAkB3IuoUJXmq85XS90VEFwLYzsyzg57isy/mfKWDAGwC0NLabgFgS4ryAmbe4iy3A/gU0mzbRkRNAcBZbk9R9iLlI6XvkJm3OX/aYgCvw23qJjVfRJQJKWTfY+ZPnN0pf2d++Sov78zJy24AkwD0QTl4X375Kgfv63QAFxPROoiZ+ndE9F8k+H2lgwDMAtCeiNoSUTUA/QF8kYqMEFEtIqpt1gH0BrDIyc9AJ9lAAJ+nIn9R8vEFgP5EVJ2I2gJoD+DnZGXK/AEcLoW8s6Tmi4gIwJsAljLzc9ahlL6zSPlK9TsjooZEVNdZrwmgF4BlSP378s1Xqt8XMw9h5hbM3AZSRk1k5uuQ6PeVqN7s8vQBcD7EO2I1gAdTmI92kJ77+QAWm7wAqA9gAoCVzrJeEvLyAaSpexBSm7gpWj4APOi8v+UA+iY5X+8CWAhggfPDb5qCfJ0BaWIvADDP+Zyf6ncWJV8pfWcATgQw17n/IgAPl/RbT3G+Uv4bs+7XE64XUELfl4aCUBRFSVPSwQSkKIqi+KACoCiKkqaoACiKoqQpKgCKoihpigqAoihKmqICoCiKkqaoACiKoqQp/w/utfGN5AZ2+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = int(len(model_trainingInfo[\"training_loss\"])/len(model_trainingInfo[\"validation_accuracy\"]))\n",
    "plt.plot(model_trainingInfo[\"training_loss\"][::steps], color = \"red\")\n",
    "plt.plot(model_trainingInfo[\"validation_accuracy\"], color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models//modelCarlos_2019.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN_Carlos(\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (drop2): Dropout(p=0.25, inplace=False)\n",
       "  (linear1_1): Linear(in_features=31, out_features=31, bias=True)\n",
       "  (linear2_1): Linear(in_features=31, out_features=10, bias=True)\n",
       "  (linear3_1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (linear1_2): Linear(in_features=31, out_features=31, bias=True)\n",
       "  (linear2_2): Linear(in_features=31, out_features=10, bias=True)\n",
       "  (linear3_2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (bn1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (out): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(modelCarlos.state_dict(), path)\n",
    "modelCarlos.load_state_dict(torch.load(path))\n",
    "modelCarlos.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018 trading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246975, 33) (123972, 33)\n",
      "(246899, 33) (123906, 33)\n"
     ]
    }
   ],
   "source": [
    "START = \"2016-01-01\"\n",
    "MIDDLE = \"2018-01-01\"\n",
    "END = \"2019-01-01\"\n",
    "training_data = data[(data[\"Unnamed: 0.1\"] >= START) & (data[\"Unnamed: 0.1\"] < MIDDLE)]\n",
    "testing_data = data[(data[\"Unnamed: 0.1\"] >= MIDDLE) & (data[\"Unnamed: 0.1\"] < END)]\n",
    "print(training_data.shape, testing_data.shape)\n",
    "\n",
    "training_data = training_data.dropna()\n",
    "testing_data = testing_data.dropna()\n",
    "print(training_data.shape, testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_training = training_data['Label'].apply(lambda x: f(x))\n",
    "Y_testing = testing_data['Label'].apply(lambda x: f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.tensor(training_data[columns].values).float()\n",
    "        self.y = torch.tensor(Y_training.values).long()\n",
    "        self.len = self.x.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class TestingData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.tensor(testing_data[columns].values).float()\n",
    "        self.y = torch.tensor(Y_testing.values).long()\n",
    "        self.len = self.x.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 31\n",
    "hidden1 = 31\n",
    "hidden2 = 10\n",
    "hidden3 = 5\n",
    "out_dim = 2\n",
    "\n",
    "model1 = DNN_Model1(input_dim, hidden1, hidden2, hidden3, out_dim)\n",
    "#model1.to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model1.parameters())\n",
    "\n",
    "training = TrainingData()\n",
    "testing = TestingData()\n",
    "train_loader = DataLoader(dataset = training, batch_size=2000, shuffle = True)\n",
    "validation_loader = DataLoader(dataset = testing, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1_trainingInfo = train(model1, criterion, train_loader, validation_loader, optimizer, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = int(len(model1_trainingInfo[\"training_loss\"])/len(model1_trainingInfo[\"validation_accuracy\"]))\n",
    "plt.plot(model1_trainingInfo[\"training_loss\"][::steps], color = \"red\")\n",
    "plt.plot(model1_trainingInfo[\"validation_accuracy\"], color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models//model1_2018.pth\"\n",
    "torch.save(model1.state_dict(), path)\n",
    "model1.load_state_dict(torch.load(path))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carlos' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 31\n",
    "hidden1 = 31\n",
    "hidden2 = 10\n",
    "hidden3 = 5\n",
    "out_dim = 2\n",
    "\n",
    "modelCarlos = DNN_Carlos(input_dim, hidden1, hidden2, hidden3, out_dim)\n",
    "#model.to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelCarlos.parameters(), lr=1e-4)\n",
    "\n",
    "training = TrainingData()\n",
    "testing = TestingData()\n",
    "train_loader = DataLoader(dataset = training, batch_size=2000, shuffle = True)\n",
    "validation_loader = DataLoader(dataset = testing, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs582/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.7116897702217102 ACCURCY: 0.5075267547363032\n",
      "current epoch 1 ,  0.25 % completed\n",
      "LOSS: 0.7040724158287048 ACCURCY: 0.5151229206349206\n",
      "current epoch 2 ,  0.5 % completed\n",
      "LOSS: 0.7155176997184753 ACCURCY: 0.518147285202253\n",
      "current epoch 3 ,  0.75 % completed\n",
      "LOSS: 0.7018017172813416 ACCURCY: 0.5173184557091653\n",
      "current epoch 4 ,  1.0 % completed\n",
      "LOSS: 0.6979820728302002 ACCURCY: 0.5217635842293907\n",
      "current epoch 5 ,  1.25 % completed\n",
      "LOSS: 0.6947671175003052 ACCURCY: 0.5255898248847926\n",
      "current epoch 6 ,  1.5 % completed\n",
      "LOSS: 0.6967142224311829 ACCURCY: 0.5297866216077828\n",
      "current epoch 7 ,  1.75 % completed\n",
      "LOSS: 0.6935055255889893 ACCURCY: 0.5310592852022529\n",
      "current epoch 8 ,  2.0 % completed\n",
      "LOSS: 0.6903125643730164 ACCURCY: 0.5358586543778803\n",
      "current epoch 9 ,  2.25 % completed\n",
      "LOSS: 0.6984395384788513 ACCURCY: 0.5372881146953404\n",
      "current epoch 10 ,  2.5 % completed\n",
      "LOSS: 0.6955699324607849 ACCURCY: 0.5386109534050179\n",
      "current epoch 11 ,  2.75 % completed\n",
      "LOSS: 0.6810581684112549 ACCURCY: 0.540938039938556\n",
      "current epoch 12 ,  3.0 % completed\n",
      "LOSS: 0.6872444152832031 ACCURCY: 0.5425633507424475\n",
      "current epoch 13 ,  3.25 % completed\n",
      "LOSS: 0.6879490613937378 ACCURCY: 0.545784712749616\n",
      "current epoch 14 ,  3.5 % completed\n",
      "LOSS: 0.6948183178901672 ACCURCY: 0.5472001146953405\n",
      "current epoch 15 ,  3.75 % completed\n",
      "LOSS: 0.6953567862510681 ACCURCY: 0.5509943881208398\n",
      "current epoch 16 ,  4.0 % completed\n",
      "LOSS: 0.6880226135253906 ACCURCY: 0.5541339068100357\n",
      "current epoch 17 ,  4.25 % completed\n",
      "LOSS: 0.6908560395240784 ACCURCY: 0.5530510189452125\n",
      "current epoch 18 ,  4.5 % completed\n",
      "LOSS: 0.6871427893638611 ACCURCY: 0.557411334357399\n",
      "current epoch 19 ,  4.75 % completed\n",
      "LOSS: 0.6937544345855713 ACCURCY: 0.5596052923707115\n",
      "current epoch 20 ,  5.0 % completed\n",
      "LOSS: 0.6833090782165527 ACCURCY: 0.5637604956477215\n",
      "current epoch 21 ,  5.25 % completed\n",
      "LOSS: 0.6830220818519592 ACCURCY: 0.5644216077828981\n",
      "current epoch 22 ,  5.5 % completed\n",
      "LOSS: 0.6881120204925537 ACCURCY: 0.5689221054787507\n",
      "current epoch 23 ,  5.75 % completed\n",
      "LOSS: 0.684111475944519 ACCURCY: 0.5724561966205837\n",
      "current epoch 24 ,  6.0 % completed\n",
      "LOSS: 0.674877941608429 ACCURCY: 0.5741611592421916\n",
      "current epoch 25 ,  6.25 % completed\n",
      "LOSS: 0.685657262802124 ACCURCY: 0.576957325140809\n",
      "current epoch 26 ,  6.5 % completed\n",
      "LOSS: 0.6731734871864319 ACCURCY: 0.5814018392217103\n",
      "current epoch 27 ,  6.75 % completed\n",
      "LOSS: 0.6857362389564514 ACCURCY: 0.5856783205325141\n",
      "current epoch 28 ,  7.0 % completed\n",
      "LOSS: 0.6766688823699951 ACCURCY: 0.5878600798771121\n",
      "current epoch 29 ,  7.25 % completed\n",
      "LOSS: 0.6698258519172668 ACCURCY: 0.5930351500256017\n",
      "current epoch 30 ,  7.5 % completed\n",
      "LOSS: 0.6604029536247253 ACCURCY: 0.5962908346134153\n",
      "current epoch 31 ,  7.75 % completed\n",
      "LOSS: 0.6661185026168823 ACCURCY: 0.6002377060931899\n",
      "current epoch 32 ,  8.0 % completed\n",
      "LOSS: 0.6716703772544861 ACCURCY: 0.604312942140297\n",
      "current epoch 33 ,  8.25 % completed\n",
      "LOSS: 0.6561416983604431 ACCURCY: 0.6043330588837684\n",
      "current epoch 34 ,  8.5 % completed\n",
      "LOSS: 0.6615273356437683 ACCURCY: 0.6108601454173068\n",
      "current epoch 35 ,  8.75 % completed\n",
      "LOSS: 0.6679832935333252 ACCURCY: 0.6109750168970816\n",
      "current epoch 36 ,  9.0 % completed\n",
      "LOSS: 0.6624906659126282 ACCURCY: 0.6158156968766002\n",
      "current epoch 37 ,  9.25 % completed\n",
      "LOSS: 0.6459912061691284 ACCURCY: 0.6151388509984641\n",
      "current epoch 38 ,  9.5 % completed\n",
      "LOSS: 0.6615657210350037 ACCURCY: 0.6221078300051205\n",
      "current epoch 39 ,  9.75 % completed\n",
      "LOSS: 0.6601166129112244 ACCURCY: 0.6204000122887865\n",
      "current epoch 40 ,  10.0 % completed\n",
      "LOSS: 0.6614991426467896 ACCURCY: 0.6236806431131592\n",
      "current epoch 41 ,  10.25 % completed\n",
      "LOSS: 0.6555367708206177 ACCURCY: 0.6268694889912954\n",
      "current epoch 42 ,  10.5 % completed\n",
      "LOSS: 0.6566796898841858 ACCURCY: 0.6304579539170505\n",
      "current epoch 43 ,  10.75 % completed\n",
      "LOSS: 0.6563700437545776 ACCURCY: 0.6339923768561188\n",
      "current epoch 44 ,  11.0 % completed\n",
      "LOSS: 0.6634715795516968 ACCURCY: 0.6344397296466974\n",
      "current epoch 45 ,  11.25 % completed\n",
      "LOSS: 0.6528417468070984 ACCURCY: 0.6353603768561189\n",
      "current epoch 46 ,  11.5 % completed\n",
      "LOSS: 0.6498672366142273 ACCURCY: 0.6376570076804916\n",
      "current epoch 47 ,  11.75 % completed\n",
      "LOSS: 0.6431980729103088 ACCURCY: 0.6400266338965693\n",
      "current epoch 48 ,  12.0 % completed\n",
      "LOSS: 0.6385005712509155 ACCURCY: 0.6423786011264722\n",
      "current epoch 49 ,  12.25 % completed\n",
      "LOSS: 0.6255136132240295 ACCURCY: 0.6432314306195596\n",
      "current epoch 50 ,  12.5 % completed\n",
      "LOSS: 0.6465911269187927 ACCURCY: 0.6463597624167946\n",
      "current epoch 51 ,  12.75 % completed\n",
      "LOSS: 0.661061704158783 ACCURCY: 0.6473440778289811\n",
      "current epoch 52 ,  13.0 % completed\n",
      "LOSS: 0.6540435552597046 ACCURCY: 0.6471507997951869\n",
      "current epoch 53 ,  13.25 % completed\n",
      "LOSS: 0.6391642689704895 ACCURCY: 0.6500372227342549\n",
      "current epoch 54 ,  13.5 % completed\n",
      "LOSS: 0.6580395102500916 ACCURCY: 0.6520080778289811\n",
      "current epoch 55 ,  13.75 % completed\n",
      "LOSS: 0.6334221959114075 ACCURCY: 0.652607463389657\n",
      "current epoch 56 ,  14.0 % completed\n",
      "LOSS: 0.6462529301643372 ACCURCY: 0.6548189656938043\n",
      "current epoch 57 ,  14.25 % completed\n",
      "LOSS: 0.6432762742042542 ACCURCY: 0.6550119447004609\n",
      "current epoch 58 ,  14.5 % completed\n",
      "LOSS: 0.630315899848938 ACCURCY: 0.6570724260112648\n",
      "current epoch 59 ,  14.75 % completed\n",
      "LOSS: 0.63043212890625 ACCURCY: 0.6564781853558627\n",
      "current epoch 60 ,  15.0 % completed\n",
      "LOSS: 0.630969226360321 ACCURCY: 0.6549268161802355\n",
      "current epoch 61 ,  15.25 % completed\n",
      "LOSS: 0.6301568150520325 ACCURCY: 0.6588861853558627\n",
      "current epoch 62 ,  15.5 % completed\n",
      "LOSS: 0.6341037154197693 ACCURCY: 0.6603585007680494\n",
      "current epoch 63 ,  15.75 % completed\n",
      "LOSS: 0.6327369213104248 ACCURCY: 0.6603440942140297\n",
      "current epoch 64 ,  16.0 % completed\n",
      "LOSS: 0.6431370377540588 ACCURCY: 0.6625636129032257\n",
      "current epoch 65 ,  16.25 % completed\n",
      "LOSS: 0.6206396818161011 ACCURCY: 0.6615543676395289\n",
      "current epoch 66 ,  16.5 % completed\n",
      "LOSS: 0.6495268940925598 ACCURCY: 0.6634583512544802\n",
      "current epoch 67 ,  16.75 % completed\n",
      "LOSS: 0.6293597221374512 ACCURCY: 0.6639681105990783\n",
      "current epoch 68 ,  17.0 % completed\n",
      "LOSS: 0.6121220588684082 ACCURCY: 0.6636401269841271\n",
      "current epoch 69 ,  17.25 % completed\n",
      "LOSS: 0.636224627494812 ACCURCY: 0.6644334050179213\n",
      "current epoch 70 ,  17.5 % completed\n",
      "LOSS: 0.6095629334449768 ACCURCY: 0.6649902017409114\n",
      "current epoch 71 ,  17.75 % completed\n",
      "LOSS: 0.6274369359016418 ACCURCY: 0.6658791643625191\n",
      "current epoch 72 ,  18.0 % completed\n",
      "LOSS: 0.6216754913330078 ACCURCY: 0.6658548325652842\n",
      "current epoch 73 ,  18.25 % completed\n",
      "LOSS: 0.6058292984962463 ACCURCY: 0.6664257204301076\n",
      "current epoch 74 ,  18.5 % completed\n",
      "LOSS: 0.621296763420105 ACCURCY: 0.6658487414234512\n",
      "current epoch 75 ,  18.75 % completed\n",
      "LOSS: 0.5949784517288208 ACCURCY: 0.6679927741935482\n",
      "current epoch 76 ,  19.0 % completed\n",
      "LOSS: 0.6589872241020203 ACCURCY: 0.6685636620583718\n",
      "current epoch 77 ,  19.25 % completed\n",
      "LOSS: 0.593080997467041 ACCURCY: 0.6698289237071173\n",
      "current epoch 78 ,  19.5 % completed\n",
      "LOSS: 0.6238258481025696 ACCURCY: 0.6688916620583718\n",
      "current epoch 79 ,  19.75 % completed\n",
      "LOSS: 0.6185781359672546 ACCURCY: 0.6689725919098823\n",
      "current epoch 80 ,  20.0 % completed\n",
      "LOSS: 0.6746367812156677 ACCURCY: 0.6693367741935483\n",
      "current epoch 81 ,  20.25 % completed\n",
      "LOSS: 0.6343362331390381 ACCURCY: 0.6701274961597544\n",
      "current epoch 82 ,  20.5 % completed\n",
      "LOSS: 0.6251708269119263 ACCURCY: 0.6696113302611368\n",
      "current epoch 83 ,  20.75 % completed\n",
      "LOSS: 0.6189088821411133 ACCURCY: 0.6710849400921657\n",
      "current epoch 84 ,  21.0 % completed\n",
      "LOSS: 0.591675341129303 ACCURCY: 0.670392458781362\n",
      "current epoch 85 ,  21.25 % completed\n",
      "LOSS: 0.6114981174468994 ACCURCY: 0.6703034961597544\n",
      "current epoch 86 ,  21.5 % completed\n",
      "LOSS: 0.6337713003158569 ACCURCY: 0.6701425171530977\n",
      "current epoch 87 ,  21.75 % completed\n",
      "LOSS: 0.6075468063354492 ACCURCY: 0.6720638115719406\n",
      "current epoch 88 ,  22.0 % completed\n",
      "LOSS: 0.6096907258033752 ACCURCY: 0.6714705171530979\n",
      "current epoch 89 ,  22.25 % completed\n",
      "LOSS: 0.6172147989273071 ACCURCY: 0.6715469984639018\n",
      "current epoch 90 ,  22.5 % completed\n",
      "LOSS: 0.616974413394928 ACCURCY: 0.6728449237071173\n",
      "current epoch 91 ,  22.75 % completed\n",
      "LOSS: 0.6187386512756348 ACCURCY: 0.6735194797747056\n",
      "current epoch 92 ,  23.0 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.6153506636619568 ACCURCY: 0.6737786830517155\n",
      "current epoch 93 ,  23.25 % completed\n",
      "LOSS: 0.6186676025390625 ACCURCY: 0.6736593302611368\n",
      "current epoch 94 ,  23.5 % completed\n",
      "LOSS: 0.6228770017623901 ACCURCY: 0.6733009400921657\n",
      "current epoch 95 ,  23.75 % completed\n",
      "LOSS: 0.6122326254844666 ACCURCY: 0.6738654214029698\n",
      "current epoch 96 ,  24.0 % completed\n",
      "LOSS: 0.6178550720214844 ACCURCY: 0.6736094050179211\n",
      "current epoch 97 ,  24.25 % completed\n",
      "LOSS: 0.6240091919898987 ACCURCY: 0.6749162928827446\n",
      "current epoch 98 ,  24.5 % completed\n",
      "LOSS: 0.592930257320404 ACCURCY: 0.6736356620583718\n",
      "current epoch 99 ,  24.75 % completed\n",
      "LOSS: 0.6245706677436829 ACCURCY: 0.6750283092677932\n",
      "current epoch 100 ,  25.0 % completed\n",
      "LOSS: 0.6245928406715393 ACCURCY: 0.6749819190988222\n",
      "current epoch 101 ,  25.25 % completed\n",
      "LOSS: 0.6164991855621338 ACCURCY: 0.6740302181259601\n",
      "current epoch 102 ,  25.5 % completed\n",
      "LOSS: 0.604153037071228 ACCURCY: 0.6753415873015873\n",
      "current epoch 103 ,  25.75 % completed\n",
      "LOSS: 0.6223931908607483 ACCURCY: 0.6744497532002047\n",
      "current epoch 104 ,  26.0 % completed\n",
      "LOSS: 0.6217510104179382 ACCURCY: 0.6756571059907833\n",
      "current epoch 105 ,  26.25 % completed\n",
      "LOSS: 0.6219868659973145 ACCURCY: 0.6757082928827445\n",
      "current epoch 106 ,  26.5 % completed\n",
      "LOSS: 0.6079877614974976 ACCURCY: 0.6749534050179211\n",
      "current epoch 107 ,  26.75 % completed\n",
      "LOSS: 0.5911493897438049 ACCURCY: 0.6754087741935483\n",
      "current epoch 108 ,  27.0 % completed\n",
      "LOSS: 0.6108816266059875 ACCURCY: 0.6754881433691754\n",
      "current epoch 109 ,  27.25 % completed\n",
      "LOSS: 0.6102802753448486 ACCURCY: 0.6760753302611368\n",
      "current epoch 110 ,  27.5 % completed\n",
      "LOSS: 0.5893034338951111 ACCURCY: 0.675904790578597\n",
      "current epoch 111 ,  27.75 % completed\n",
      "LOSS: 0.6160800457000732 ACCURCY: 0.6765524751664107\n",
      "current epoch 112 ,  28.0 % completed\n",
      "LOSS: 0.6123279333114624 ACCURCY: 0.6762142345110088\n",
      "current epoch 113 ,  28.25 % completed\n",
      "LOSS: 0.6198081970214844 ACCURCY: 0.6763012718894008\n",
      "current epoch 114 ,  28.5 % completed\n",
      "LOSS: 0.6139637231826782 ACCURCY: 0.6766615873015874\n",
      "current epoch 115 ,  28.75 % completed\n",
      "LOSS: 0.6020140647888184 ACCURCY: 0.6769102345110087\n",
      "current epoch 116 ,  29.0 % completed\n",
      "LOSS: 0.6160548329353333 ACCURCY: 0.6773143840245776\n",
      "current epoch 117 ,  29.25 % completed\n",
      "LOSS: 0.6078482270240784 ACCURCY: 0.6768887741935482\n",
      "current epoch 118 ,  29.5 % completed\n",
      "LOSS: 0.5977181792259216 ACCURCY: 0.6771304004096261\n",
      "current epoch 119 ,  29.75 % completed\n",
      "LOSS: 0.6241164207458496 ACCURCY: 0.6765559938556069\n",
      "current epoch 120 ,  30.0 % completed\n",
      "LOSS: 0.6206168532371521 ACCURCY: 0.6765127905785969\n",
      "current epoch 121 ,  30.25 % completed\n",
      "LOSS: 0.6107827425003052 ACCURCY: 0.6774567741935484\n",
      "current epoch 122 ,  30.5 % completed\n",
      "LOSS: 0.6003785133361816 ACCURCY: 0.6772430312339991\n",
      "current epoch 123 ,  30.75 % completed\n",
      "LOSS: 0.6074552536010742 ACCURCY: 0.6769988817204302\n",
      "current epoch 124 ,  31.0 % completed\n",
      "LOSS: 0.6051650643348694 ACCURCY: 0.67716142140297\n",
      "current epoch 125 ,  31.25 % completed\n",
      "LOSS: 0.6006726622581482 ACCURCY: 0.6784417532002049\n",
      "current epoch 126 ,  31.5 % completed\n",
      "LOSS: 0.5838543176651001 ACCURCY: 0.677791180747568\n",
      "current epoch 127 ,  31.75 % completed\n",
      "LOSS: 0.6308677792549133 ACCURCY: 0.677905753200205\n",
      "current epoch 128 ,  32.0 % completed\n",
      "LOSS: 0.6145219802856445 ACCURCY: 0.677544790578597\n",
      "current epoch 129 ,  32.25 % completed\n",
      "LOSS: 0.5935839414596558 ACCURCY: 0.6787412718894009\n",
      "current epoch 130 ,  32.5 % completed\n",
      "LOSS: 0.6120191216468811 ACCURCY: 0.67814542140297\n",
      "current epoch 131 ,  32.75 % completed\n",
      "LOSS: 0.6050294637680054 ACCURCY: 0.6779847905785971\n",
      "current epoch 132 ,  33.0 % completed\n",
      "LOSS: 0.6017051339149475 ACCURCY: 0.6770987158218127\n",
      "current epoch 133 ,  33.25 % completed\n",
      "LOSS: 0.6055746078491211 ACCURCY: 0.6779028653353815\n",
      "current epoch 134 ,  33.5 % completed\n",
      "LOSS: 0.6090257167816162 ACCURCY: 0.6775464004096263\n",
      "current epoch 135 ,  33.75 % completed\n",
      "LOSS: 0.6354402899742126 ACCURCY: 0.6782471807475678\n",
      "current epoch 136 ,  34.0 % completed\n",
      "LOSS: 0.5860045552253723 ACCURCY: 0.6784343840245775\n",
      "current epoch 137 ,  34.25 % completed\n",
      "LOSS: 0.6127640008926392 ACCURCY: 0.6796231971326163\n",
      "current epoch 138 ,  34.5 % completed\n",
      "LOSS: 0.6041687726974487 ACCURCY: 0.6788526410650281\n",
      "current epoch 139 ,  34.75 % completed\n",
      "LOSS: 0.6096023917198181 ACCURCY: 0.6787028653353815\n",
      "current epoch 140 ,  35.0 % completed\n",
      "LOSS: 0.6147975325584412 ACCURCY: 0.6782619190988224\n",
      "current epoch 141 ,  35.25 % completed\n",
      "LOSS: 0.5945208072662354 ACCURCY: 0.6785534377880182\n",
      "current epoch 142 ,  35.5 % completed\n",
      "LOSS: 0.6109192371368408 ACCURCY: 0.6780743840245776\n",
      "current epoch 143 ,  35.75 % completed\n",
      "LOSS: 0.5902255773544312 ACCURCY: 0.6797239938556068\n",
      "current epoch 144 ,  36.0 % completed\n",
      "LOSS: 0.5887685418128967 ACCURCY: 0.6785809564772144\n",
      "current epoch 145 ,  36.25 % completed\n",
      "LOSS: 0.6124159693717957 ACCURCY: 0.6798026994367641\n",
      "current epoch 146 ,  36.5 % completed\n",
      "LOSS: 0.5853998064994812 ACCURCY: 0.6791073466461854\n",
      "current epoch 147 ,  36.75 % completed\n",
      "LOSS: 0.6033568382263184 ACCURCY: 0.6792724751664106\n",
      "current epoch 148 ,  37.0 % completed\n",
      "LOSS: 0.6066498160362244 ACCURCY: 0.6787118115719406\n",
      "current epoch 149 ,  37.25 % completed\n",
      "LOSS: 0.5924156308174133 ACCURCY: 0.6787854377880186\n",
      "current epoch 150 ,  37.5 % completed\n",
      "LOSS: 0.5929028987884521 ACCURCY: 0.6790027158218125\n",
      "current epoch 151 ,  37.75 % completed\n",
      "LOSS: 0.620890200138092 ACCURCY: 0.678469271889401\n",
      "current epoch 152 ,  38.0 % completed\n",
      "LOSS: 0.6073780655860901 ACCURCY: 0.6797191971326164\n",
      "current epoch 153 ,  38.25 % completed\n",
      "LOSS: 0.6022017002105713 ACCURCY: 0.6794721597542244\n",
      "current epoch 154 ,  38.5 % completed\n",
      "LOSS: 0.6066538095474243 ACCURCY: 0.6798987158218125\n",
      "current epoch 155 ,  38.75 % completed\n",
      "LOSS: 0.5855903625488281 ACCURCY: 0.6788337532002048\n",
      "current epoch 156 ,  39.0 % completed\n",
      "LOSS: 0.6196488738059998 ACCURCY: 0.6803163256528418\n",
      "current epoch 157 ,  39.25 % completed\n",
      "LOSS: 0.5999822616577148 ACCURCY: 0.6797182345110088\n",
      "current epoch 158 ,  39.5 % completed\n",
      "LOSS: 0.6075523495674133 ACCURCY: 0.6798699190988224\n",
      "current epoch 159 ,  39.75 % completed\n",
      "LOSS: 0.5895023941993713 ACCURCY: 0.6797575873015874\n",
      "current epoch 160 ,  40.0 % completed\n",
      "LOSS: 0.5888219475746155 ACCURCY: 0.6797710312339987\n",
      "current epoch 161 ,  40.25 % completed\n",
      "LOSS: 0.6037309765815735 ACCURCY: 0.6797073466461855\n",
      "current epoch 162 ,  40.5 % completed\n",
      "LOSS: 0.5968224406242371 ACCURCY: 0.6793947158218125\n",
      "current epoch 163 ,  40.75 % completed\n",
      "LOSS: 0.6159759163856506 ACCURCY: 0.6803704004096263\n",
      "current epoch 164 ,  41.0 % completed\n",
      "LOSS: 0.6078827977180481 ACCURCY: 0.6794238443420378\n",
      "current epoch 165 ,  41.25 % completed\n",
      "LOSS: 0.6029924750328064 ACCURCY: 0.6796344004096264\n",
      "current epoch 166 ,  41.5 % completed\n",
      "LOSS: 0.5972106456756592 ACCURCY: 0.6798299190988224\n",
      "current epoch 167 ,  41.75 % completed\n",
      "LOSS: 0.6163910627365112 ACCURCY: 0.6804817532002048\n",
      "current epoch 168 ,  42.0 % completed\n",
      "LOSS: 0.6080989241600037 ACCURCY: 0.6808504004096261\n",
      "current epoch 169 ,  42.25 % completed\n",
      "LOSS: 0.6074186563491821 ACCURCY: 0.6801531223758319\n",
      "current epoch 170 ,  42.5 % completed\n",
      "LOSS: 0.5973381996154785 ACCURCY: 0.6792500686123911\n",
      "current epoch 171 ,  42.75 % completed\n",
      "LOSS: 0.6174487471580505 ACCURCY: 0.6799374377880184\n",
      "current epoch 172 ,  43.0 % completed\n",
      "LOSS: 0.6081562042236328 ACCURCY: 0.6802266994367637\n",
      "current epoch 173 ,  43.25 % completed\n",
      "LOSS: 0.6094639897346497 ACCURCY: 0.6801316784434205\n",
      "current epoch 174 ,  43.5 % completed\n",
      "LOSS: 0.5818665623664856 ACCURCY: 0.6812481761392729\n",
      "current epoch 175 ,  43.75 % completed\n",
      "LOSS: 0.6052901744842529 ACCURCY: 0.6799966246799793\n",
      "current epoch 176 ,  44.0 % completed\n",
      "LOSS: 0.5995364785194397 ACCURCY: 0.6796007905785969\n",
      "current epoch 177 ,  44.25 % completed\n",
      "LOSS: 0.6160275340080261 ACCURCY: 0.6804820849974398\n",
      "current epoch 178 ,  44.5 % completed\n",
      "LOSS: 0.5918228030204773 ACCURCY: 0.680061271889401\n",
      "current epoch 179 ,  44.75 % completed\n",
      "LOSS: 0.5755382180213928 ACCURCY: 0.6804334377880187\n",
      "current epoch 180 ,  45.0 % completed\n",
      "LOSS: 0.6105120182037354 ACCURCY: 0.6796561433691757\n",
      "current epoch 181 ,  45.25 % completed\n",
      "LOSS: 0.6095532774925232 ACCURCY: 0.6807940849974399\n",
      "current epoch 182 ,  45.5 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.6084160804748535 ACCURCY: 0.6806587158218126\n",
      "current epoch 183 ,  45.75 % completed\n",
      "LOSS: 0.5785248875617981 ACCURCY: 0.6799035125448029\n",
      "current epoch 184 ,  46.0 % completed\n",
      "LOSS: 0.5846149325370789 ACCURCY: 0.6805716784434205\n",
      "current epoch 185 ,  46.25 % completed\n",
      "LOSS: 0.5948612093925476 ACCURCY: 0.6807214377880186\n",
      "current epoch 186 ,  46.5 % completed\n",
      "LOSS: 0.6009072661399841 ACCURCY: 0.6803160102406555\n",
      "current epoch 187 ,  46.75 % completed\n",
      "LOSS: 0.6151339411735535 ACCURCY: 0.6800273630312339\n",
      "current epoch 188 ,  47.0 % completed\n",
      "LOSS: 0.5968437194824219 ACCURCY: 0.680579363031234\n",
      "current epoch 189 ,  47.25 % completed\n",
      "LOSS: 0.5988768935203552 ACCURCY: 0.6815579190988224\n",
      "current epoch 190 ,  47.5 % completed\n",
      "LOSS: 0.611204981803894 ACCURCY: 0.6811121597542243\n",
      "current epoch 191 ,  47.75 % completed\n",
      "LOSS: 0.5956111550331116 ACCURCY: 0.6803748817204301\n",
      "current epoch 192 ,  48.0 % completed\n",
      "LOSS: 0.5975518226623535 ACCURCY: 0.6809851059907834\n",
      "current epoch 193 ,  48.25 % completed\n",
      "LOSS: 0.5890268087387085 ACCURCY: 0.6811649564772146\n",
      "current epoch 194 ,  48.5 % completed\n",
      "LOSS: 0.59991455078125 ACCURCY: 0.6803153630312339\n",
      "current epoch 195 ,  48.75 % completed\n",
      "LOSS: 0.58692467212677 ACCURCY: 0.6806215873015873\n",
      "current epoch 196 ,  49.0 % completed\n",
      "LOSS: 0.6076129078865051 ACCURCY: 0.6805483092677932\n",
      "current epoch 197 ,  49.25 % completed\n",
      "LOSS: 0.5872164964675903 ACCURCY: 0.6802884915514592\n",
      "current epoch 198 ,  49.5 % completed\n",
      "LOSS: 0.5963088870048523 ACCURCY: 0.6812555125448029\n",
      "current epoch 199 ,  49.75 % completed\n",
      "LOSS: 0.5942657589912415 ACCURCY: 0.6807316784434205\n",
      "current epoch 200 ,  50.0 % completed\n",
      "LOSS: 0.5866186618804932 ACCURCY: 0.6811144004096262\n",
      "current epoch 201 ,  50.25 % completed\n",
      "LOSS: 0.587754487991333 ACCURCY: 0.6807585499231951\n",
      "current epoch 202 ,  50.5 % completed\n",
      "LOSS: 0.6306912899017334 ACCURCY: 0.680883031233999\n",
      "current epoch 203 ,  50.75 % completed\n",
      "LOSS: 0.5904457569122314 ACCURCY: 0.6808411223758318\n",
      "current epoch 204 ,  51.0 % completed\n",
      "LOSS: 0.6024150848388672 ACCURCY: 0.6796744004096261\n",
      "current epoch 205 ,  51.25 % completed\n",
      "LOSS: 0.5935291647911072 ACCURCY: 0.6815851223758319\n",
      "current epoch 206 ,  51.5 % completed\n",
      "LOSS: 0.5973598957061768 ACCURCY: 0.6817150476190476\n",
      "current epoch 207 ,  51.75 % completed\n",
      "LOSS: 0.595399022102356 ACCURCY: 0.6804523256528419\n",
      "current epoch 208 ,  52.0 % completed\n",
      "LOSS: 0.6034155488014221 ACCURCY: 0.6817150476190476\n",
      "current epoch 209 ,  52.25 % completed\n",
      "LOSS: 0.5958545207977295 ACCURCY: 0.6814417695852534\n",
      "current epoch 210 ,  52.5 % completed\n",
      "LOSS: 0.6052865386009216 ACCURCY: 0.6813985663082436\n",
      "current epoch 211 ,  52.75 % completed\n",
      "LOSS: 0.5941376686096191 ACCURCY: 0.6809259190988223\n",
      "current epoch 212 ,  53.0 % completed\n",
      "LOSS: 0.6100521683692932 ACCURCY: 0.6815694377880186\n",
      "current epoch 213 ,  53.25 % completed\n",
      "LOSS: 0.5931661128997803 ACCURCY: 0.6811966410650281\n",
      "current epoch 214 ,  53.5 % completed\n",
      "LOSS: 0.594232439994812 ACCURCY: 0.6817019190988225\n",
      "current epoch 215 ,  53.75 % completed\n",
      "LOSS: 0.5894199013710022 ACCURCY: 0.681075363031234\n",
      "current epoch 216 ,  54.0 % completed\n",
      "LOSS: 0.590979278087616 ACCURCY: 0.6819160102406555\n",
      "current epoch 217 ,  54.25 % completed\n",
      "LOSS: 0.5821231007575989 ACCURCY: 0.680477603686636\n",
      "current epoch 218 ,  54.5 % completed\n",
      "LOSS: 0.6090502738952637 ACCURCY: 0.682451694828469\n",
      "current epoch 219 ,  54.75 % completed\n",
      "LOSS: 0.59537273645401 ACCURCY: 0.6816507322068612\n",
      "current epoch 220 ,  55.0 % completed\n",
      "LOSS: 0.5986924171447754 ACCURCY: 0.6811211059907833\n",
      "current epoch 221 ,  55.25 % completed\n",
      "LOSS: 0.5933339595794678 ACCURCY: 0.680875363031234\n",
      "current epoch 222 ,  55.5 % completed\n",
      "LOSS: 0.6050577163696289 ACCURCY: 0.6815742508960574\n",
      "current epoch 223 ,  55.75 % completed\n",
      "LOSS: 0.5855634212493896 ACCURCY: 0.6809966246799796\n",
      "current epoch 224 ,  56.0 % completed\n",
      "LOSS: 0.6020649671554565 ACCURCY: 0.6817831971326165\n",
      "current epoch 225 ,  56.25 % completed\n",
      "LOSS: 0.5911909341812134 ACCURCY: 0.680994400409626\n",
      "current epoch 226 ,  56.5 % completed\n",
      "LOSS: 0.6114745140075684 ACCURCY: 0.6813835289298514\n",
      "current epoch 227 ,  56.75 % completed\n",
      "LOSS: 0.5882586240768433 ACCURCY: 0.6812804915514593\n",
      "current epoch 228 ,  57.0 % completed\n",
      "LOSS: 0.6090890765190125 ACCURCY: 0.6817188981054788\n",
      "current epoch 229 ,  57.25 % completed\n",
      "LOSS: 0.5915000438690186 ACCURCY: 0.6809998443420379\n",
      "current epoch 230 ,  57.5 % completed\n",
      "LOSS: 0.6038474440574646 ACCURCY: 0.6818248069636457\n",
      "current epoch 231 ,  57.75 % completed\n",
      "LOSS: 0.60328209400177 ACCURCY: 0.6810363256528419\n",
      "current epoch 232 ,  58.0 % completed\n",
      "LOSS: 0.5962530374526978 ACCURCY: 0.6810254377880185\n",
      "current epoch 233 ,  58.25 % completed\n",
      "LOSS: 0.595941424369812 ACCURCY: 0.6818913630312339\n",
      "current epoch 234 ,  58.5 % completed\n",
      "LOSS: 0.564607560634613 ACCURCY: 0.6816168069636457\n",
      "current epoch 235 ,  58.75 % completed\n",
      "LOSS: 0.5786682963371277 ACCURCY: 0.6815972718894008\n",
      "current epoch 236 ,  59.0 % completed\n",
      "LOSS: 0.5927966833114624 ACCURCY: 0.6815351971326165\n",
      "current epoch 237 ,  59.25 % completed\n",
      "LOSS: 0.6041422486305237 ACCURCY: 0.6806724751664106\n",
      "current epoch 238 ,  59.5 % completed\n",
      "LOSS: 0.5932235717773438 ACCURCY: 0.6810017695852535\n",
      "current epoch 239 ,  59.75 % completed\n",
      "LOSS: 0.607780396938324 ACCURCY: 0.6813560102406554\n",
      "current epoch 240 ,  60.0 % completed\n",
      "LOSS: 0.5855059623718262 ACCURCY: 0.6813700849974399\n",
      "current epoch 241 ,  60.25 % completed\n",
      "LOSS: 0.6079531311988831 ACCURCY: 0.6823220849974398\n",
      "current epoch 242 ,  60.5 % completed\n",
      "LOSS: 0.5896254181861877 ACCURCY: 0.6811144004096262\n",
      "current epoch 243 ,  60.75 % completed\n",
      "LOSS: 0.5737722516059875 ACCURCY: 0.6814347322068612\n",
      "current epoch 244 ,  61.0 % completed\n",
      "LOSS: 0.6074197292327881 ACCURCY: 0.6807070476190478\n",
      "current epoch 245 ,  61.25 % completed\n",
      "LOSS: 0.6140543222427368 ACCURCY: 0.6815521597542242\n",
      "current epoch 246 ,  61.5 % completed\n",
      "LOSS: 0.5918651223182678 ACCURCY: 0.6814974377880185\n",
      "current epoch 247 ,  61.75 % completed\n",
      "LOSS: 0.6022583842277527 ACCURCY: 0.6804951971326164\n",
      "current epoch 248 ,  62.0 % completed\n",
      "LOSS: 0.5890607237815857 ACCURCY: 0.6816859190988224\n",
      "current epoch 249 ,  62.25 % completed\n",
      "LOSS: 0.5938650369644165 ACCURCY: 0.6826788981054787\n",
      "current epoch 250 ,  62.5 % completed\n",
      "LOSS: 0.5944503545761108 ACCURCY: 0.68191408499744\n",
      "current epoch 251 ,  62.75 % completed\n",
      "LOSS: 0.5892744660377502 ACCURCY: 0.6811854377880183\n",
      "current epoch 252 ,  63.0 % completed\n",
      "LOSS: 0.5939077138900757 ACCURCY: 0.6810033630312339\n",
      "current epoch 253 ,  63.25 % completed\n",
      "LOSS: 0.6042095422744751 ACCURCY: 0.681191844342038\n",
      "current epoch 254 ,  63.5 % completed\n",
      "LOSS: 0.6048621535301208 ACCURCY: 0.6814356784434205\n",
      "current epoch 255 ,  63.75 % completed\n",
      "LOSS: 0.5929802656173706 ACCURCY: 0.6822785663082436\n",
      "current epoch 256 ,  64.0 % completed\n",
      "LOSS: 0.5911572575569153 ACCURCY: 0.6821144004096262\n",
      "current epoch 257 ,  64.25 % completed\n",
      "LOSS: 0.5973271131515503 ACCURCY: 0.6813857695852537\n",
      "current epoch 258 ,  64.5 % completed\n",
      "LOSS: 0.6080948114395142 ACCURCY: 0.6813652882744496\n",
      "current epoch 259 ,  64.75 % completed\n",
      "LOSS: 0.6090863943099976 ACCURCY: 0.6827736036866358\n",
      "current epoch 260 ,  65.0 % completed\n",
      "LOSS: 0.5929217338562012 ACCURCY: 0.6818219190988224\n",
      "current epoch 261 ,  65.25 % completed\n",
      "LOSS: 0.611030101776123 ACCURCY: 0.6815518443420377\n",
      "current epoch 262 ,  65.5 % completed\n",
      "LOSS: 0.6012592911720276 ACCURCY: 0.6812417695852535\n",
      "current epoch 263 ,  65.75 % completed\n",
      "LOSS: 0.5846665501594543 ACCURCY: 0.6822347322068613\n",
      "current epoch 264 ,  66.0 % completed\n",
      "LOSS: 0.5845377445220947 ACCURCY: 0.6821332718894009\n",
      "current epoch 265 ,  66.25 % completed\n",
      "LOSS: 0.5838374495506287 ACCURCY: 0.6815422508960574\n",
      "current epoch 266 ,  66.5 % completed\n",
      "LOSS: 0.5804983973503113 ACCURCY: 0.6810929564772146\n",
      "current epoch 267 ,  66.75 % completed\n",
      "LOSS: 0.5902760028839111 ACCURCY: 0.681457122375832\n",
      "current epoch 268 ,  67.0 % completed\n",
      "LOSS: 0.5745840072631836 ACCURCY: 0.6812561597542242\n",
      "current epoch 269 ,  67.25 % completed\n",
      "LOSS: 0.6013256311416626 ACCURCY: 0.6822155289298516\n",
      "current epoch 270 ,  67.5 % completed\n",
      "LOSS: 0.6062477231025696 ACCURCY: 0.6817652882744496\n",
      "current epoch 271 ,  67.75 % completed\n",
      "LOSS: 0.6058499813079834 ACCURCY: 0.6813905663082438\n",
      "current epoch 272 ,  68.0 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.6075479388237 ACCURCY: 0.6819716948284689\n",
      "current epoch 273 ,  68.25 % completed\n",
      "LOSS: 0.5946826338768005 ACCURCY: 0.6812097695852535\n",
      "current epoch 274 ,  68.5 % completed\n",
      "LOSS: 0.5925993323326111 ACCURCY: 0.6819348817204304\n",
      "current epoch 275 ,  68.75 % completed\n",
      "LOSS: 0.5980512499809265 ACCURCY: 0.6820430476190477\n",
      "current epoch 276 ,  69.0 % completed\n",
      "LOSS: 0.5912654995918274 ACCURCY: 0.6827448069636456\n",
      "current epoch 277 ,  69.25 % completed\n",
      "LOSS: 0.6012781858444214 ACCURCY: 0.682437603686636\n",
      "current epoch 278 ,  69.5 % completed\n",
      "LOSS: 0.6156699657440186 ACCURCY: 0.6806411223758319\n",
      "current epoch 279 ,  69.75 % completed\n",
      "LOSS: 0.5968935489654541 ACCURCY: 0.6816561597542242\n",
      "current epoch 280 ,  70.0 % completed\n",
      "LOSS: 0.5767029523849487 ACCURCY: 0.6815985663082437\n",
      "current epoch 281 ,  70.25 % completed\n",
      "LOSS: 0.6231469511985779 ACCURCY: 0.6821585663082438\n",
      "current epoch 282 ,  70.5 % completed\n",
      "LOSS: 0.5852063894271851 ACCURCY: 0.6822059190988223\n",
      "current epoch 283 ,  70.75 % completed\n",
      "LOSS: 0.5805152058601379 ACCURCY: 0.6811070476190476\n",
      "current epoch 284 ,  71.0 % completed\n",
      "LOSS: 0.6174596548080444 ACCURCY: 0.6815774377880186\n",
      "current epoch 285 ,  71.25 % completed\n",
      "LOSS: 0.5873509645462036 ACCURCY: 0.6821083256528417\n",
      "current epoch 286 ,  71.5 % completed\n",
      "LOSS: 0.6094464063644409 ACCURCY: 0.6825483256528417\n",
      "current epoch 287 ,  71.75 % completed\n",
      "LOSS: 0.5936474800109863 ACCURCY: 0.6821502508960574\n",
      "current epoch 288 ,  72.0 % completed\n",
      "LOSS: 0.607329249382019 ACCURCY: 0.6824270476190476\n",
      "current epoch 289 ,  72.25 % completed\n",
      "LOSS: 0.6022576093673706 ACCURCY: 0.6822283256528417\n",
      "current epoch 290 ,  72.5 % completed\n",
      "LOSS: 0.6031591892242432 ACCURCY: 0.6825492882744496\n",
      "current epoch 291 ,  72.75 % completed\n",
      "LOSS: 0.5848883390426636 ACCURCY: 0.6826830476190477\n",
      "current epoch 292 ,  73.0 % completed\n",
      "LOSS: 0.594293475151062 ACCURCY: 0.681782566308244\n",
      "current epoch 293 ,  73.25 % completed\n",
      "LOSS: 0.6006160974502563 ACCURCY: 0.6818356784434204\n",
      "current epoch 294 ,  73.5 % completed\n",
      "LOSS: 0.5898634791374207 ACCURCY: 0.6816718443420381\n",
      "current epoch 295 ,  73.75 % completed\n",
      "LOSS: 0.5787509679794312 ACCURCY: 0.6823092882744496\n",
      "current epoch 296 ,  74.0 % completed\n",
      "LOSS: 0.6132346987724304 ACCURCY: 0.6818971223758319\n",
      "current epoch 297 ,  74.25 % completed\n",
      "LOSS: 0.6051881909370422 ACCURCY: 0.6819460849974399\n",
      "current epoch 298 ,  74.5 % completed\n",
      "LOSS: 0.6115683317184448 ACCURCY: 0.6811716948284691\n",
      "current epoch 299 ,  74.75 % completed\n",
      "LOSS: 0.5859681963920593 ACCURCY: 0.6818676948284689\n",
      "current epoch 300 ,  75.0 % completed\n",
      "LOSS: 0.5793548226356506 ACCURCY: 0.6828168069636458\n",
      "current epoch 301 ,  75.25 % completed\n",
      "LOSS: 0.5960668921470642 ACCURCY: 0.6822228817204299\n",
      "current epoch 302 ,  75.5 % completed\n",
      "LOSS: 0.6090778112411499 ACCURCY: 0.6816561597542242\n",
      "current epoch 303 ,  75.75 % completed\n",
      "LOSS: 0.5931790471076965 ACCURCY: 0.6822404751664108\n",
      "current epoch 304 ,  76.0 % completed\n",
      "LOSS: 0.5822511315345764 ACCURCY: 0.682139363031234\n",
      "current epoch 305 ,  76.25 % completed\n",
      "LOSS: 0.5966622233390808 ACCURCY: 0.6824302345110087\n",
      "current epoch 306 ,  76.5 % completed\n",
      "LOSS: 0.6100444197654724 ACCURCY: 0.6822270476190475\n",
      "current epoch 307 ,  76.75 % completed\n",
      "LOSS: 0.6039918661117554 ACCURCY: 0.682531363031234\n",
      "current epoch 308 ,  77.0 % completed\n",
      "LOSS: 0.6049610376358032 ACCURCY: 0.682398549923195\n",
      "current epoch 309 ,  77.25 % completed\n",
      "LOSS: 0.5942705869674683 ACCURCY: 0.6821659190988223\n",
      "current epoch 310 ,  77.5 % completed\n",
      "LOSS: 0.5614334940910339 ACCURCY: 0.6816712135176651\n",
      "current epoch 311 ,  77.75 % completed\n",
      "LOSS: 0.5888036489486694 ACCURCY: 0.6825563256528417\n",
      "current epoch 312 ,  78.0 % completed\n",
      "LOSS: 0.5914376974105835 ACCURCY: 0.6818539190988224\n",
      "current epoch 313 ,  78.25 % completed\n",
      "LOSS: 0.5800365805625916 ACCURCY: 0.6821710476190477\n",
      "current epoch 314 ,  78.5 % completed\n",
      "LOSS: 0.5922913551330566 ACCURCY: 0.6822708817204303\n",
      "current epoch 315 ,  78.75 % completed\n",
      "LOSS: 0.5849913358688354 ACCURCY: 0.6830212882744494\n",
      "current epoch 316 ,  79.0 % completed\n",
      "LOSS: 0.6134276390075684 ACCURCY: 0.6824852882744495\n",
      "current epoch 317 ,  79.25 % completed\n",
      "LOSS: 0.5781941413879395 ACCURCY: 0.68181008499744\n",
      "current epoch 318 ,  79.5 % completed\n",
      "LOSS: 0.5912564992904663 ACCURCY: 0.6819185663082438\n",
      "current epoch 319 ,  79.75 % completed\n",
      "LOSS: 0.581188440322876 ACCURCY: 0.6826155289298513\n",
      "current epoch 320 ,  80.0 % completed\n",
      "LOSS: 0.5843735933303833 ACCURCY: 0.6826097695852534\n",
      "current epoch 321 ,  80.25 % completed\n",
      "LOSS: 0.5854552388191223 ACCURCY: 0.680923363031234\n",
      "current epoch 322 ,  80.5 % completed\n",
      "LOSS: 0.598869264125824 ACCURCY: 0.6822744004096262\n",
      "current epoch 323 ,  80.75 % completed\n",
      "LOSS: 0.5795676708221436 ACCURCY: 0.6821185663082437\n",
      "current epoch 324 ,  81.0 % completed\n",
      "LOSS: 0.626457691192627 ACCURCY: 0.681835363031234\n",
      "current epoch 325 ,  81.25 % completed\n",
      "LOSS: 0.6076035499572754 ACCURCY: 0.6825188817204301\n",
      "current epoch 326 ,  81.5 % completed\n",
      "LOSS: 0.598277747631073 ACCURCY: 0.6821585663082438\n",
      "current epoch 327 ,  81.75 % completed\n",
      "LOSS: 0.6134013533592224 ACCURCY: 0.6831857695852535\n",
      "current epoch 328 ,  82.0 % completed\n",
      "LOSS: 0.5979066491127014 ACCURCY: 0.6824564915514594\n",
      "current epoch 329 ,  82.25 % completed\n",
      "LOSS: 0.6248416304588318 ACCURCY: 0.6821371223758321\n",
      "current epoch 330 ,  82.5 % completed\n",
      "LOSS: 0.5985125303268433 ACCURCY: 0.6822088069636456\n",
      "current epoch 331 ,  82.75 % completed\n",
      "LOSS: 0.5957030057907104 ACCURCY: 0.6826958443420381\n",
      "current epoch 332 ,  83.0 % completed\n",
      "LOSS: 0.6155738830566406 ACCURCY: 0.6828920102406555\n",
      "current epoch 333 ,  83.25 % completed\n",
      "LOSS: 0.6046971082687378 ACCURCY: 0.6820606410650282\n",
      "current epoch 334 ,  83.5 % completed\n",
      "LOSS: 0.6059576869010925 ACCURCY: 0.6829841761392729\n",
      "current epoch 335 ,  83.75 % completed\n",
      "LOSS: 0.5887625813484192 ACCURCY: 0.6827537695852536\n",
      "current epoch 336 ,  84.0 % completed\n",
      "LOSS: 0.590768039226532 ACCURCY: 0.6816222508960574\n",
      "current epoch 337 ,  84.25 % completed\n",
      "LOSS: 0.598847508430481 ACCURCY: 0.6822827322068613\n",
      "current epoch 338 ,  84.5 % completed\n",
      "LOSS: 0.596208930015564 ACCURCY: 0.6823054541730671\n",
      "current epoch 339 ,  84.75 % completed\n",
      "LOSS: 0.6049003005027771 ACCURCY: 0.6817515289298514\n",
      "current epoch 340 ,  85.0 % completed\n",
      "LOSS: 0.569021999835968 ACCURCY: 0.6824721597542244\n",
      "current epoch 341 ,  85.25 % completed\n",
      "LOSS: 0.6064120531082153 ACCURCY: 0.6821156784434205\n",
      "current epoch 342 ,  85.5 % completed\n",
      "LOSS: 0.5998331308364868 ACCURCY: 0.6822875289298512\n",
      "current epoch 343 ,  85.75 % completed\n",
      "LOSS: 0.579265296459198 ACCURCY: 0.682557603686636\n",
      "current epoch 344 ,  86.0 % completed\n",
      "LOSS: 0.5866601467132568 ACCURCY: 0.683210416794675\n",
      "current epoch 345 ,  86.25 % completed\n",
      "LOSS: 0.5815450549125671 ACCURCY: 0.681916641065028\n",
      "current epoch 346 ,  86.5 % completed\n",
      "LOSS: 0.5950044989585876 ACCURCY: 0.6818785663082436\n",
      "current epoch 347 ,  86.75 % completed\n",
      "LOSS: 0.5813437700271606 ACCURCY: 0.6823022508960573\n",
      "current epoch 348 ,  87.0 % completed\n",
      "LOSS: 0.5774644017219543 ACCURCY: 0.6822795289298516\n",
      "current epoch 349 ,  87.25 % completed\n",
      "LOSS: 0.6063383221626282 ACCURCY: 0.6831105663082438\n",
      "current epoch 350 ,  87.5 % completed\n",
      "LOSS: 0.5645803809165955 ACCURCY: 0.6818555289298512\n",
      "current epoch 351 ,  87.75 % completed\n",
      "LOSS: 0.5744849443435669 ACCURCY: 0.6824235289298515\n",
      "current epoch 352 ,  88.0 % completed\n",
      "LOSS: 0.5859899520874023 ACCURCY: 0.6821496036866361\n",
      "current epoch 353 ,  88.25 % completed\n",
      "LOSS: 0.5947399139404297 ACCURCY: 0.6821470476190475\n",
      "current epoch 354 ,  88.5 % completed\n",
      "LOSS: 0.5989592671394348 ACCURCY: 0.68215888172043\n",
      "current epoch 355 ,  88.75 % completed\n",
      "LOSS: 0.5887243151664734 ACCURCY: 0.6821723256528419\n",
      "current epoch 356 ,  89.0 % completed\n",
      "LOSS: 0.5979322195053101 ACCURCY: 0.6824452882744494\n",
      "current epoch 357 ,  89.25 % completed\n",
      "LOSS: 0.6014437675476074 ACCURCY: 0.6823924915514594\n",
      "current epoch 358 ,  89.5 % completed\n",
      "LOSS: 0.6163954138755798 ACCURCY: 0.6823156784434203\n",
      "current epoch 359 ,  89.75 % completed\n",
      "LOSS: 0.5956106781959534 ACCURCY: 0.681649122375832\n",
      "current epoch 360 ,  90.0 % completed\n",
      "LOSS: 0.5932581424713135 ACCURCY: 0.6825191971326164\n",
      "current epoch 361 ,  90.25 % completed\n",
      "LOSS: 0.5828686952590942 ACCURCY: 0.682677935483871\n",
      "current epoch 362 ,  90.5 % completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.5777817964553833 ACCURCY: 0.6827681597542243\n",
      "current epoch 363 ,  90.75 % completed\n",
      "LOSS: 0.5890198945999146 ACCURCY: 0.6822452882744494\n",
      "current epoch 364 ,  91.0 % completed\n",
      "LOSS: 0.5989985466003418 ACCURCY: 0.682447213517665\n",
      "current epoch 365 ,  91.25 % completed\n",
      "LOSS: 0.605715274810791 ACCURCY: 0.6824283256528417\n",
      "current epoch 366 ,  91.5 % completed\n",
      "LOSS: 0.5818501114845276 ACCURCY: 0.6821518443420379\n",
      "current epoch 367 ,  91.75 % completed\n",
      "LOSS: 0.5802417397499084 ACCURCY: 0.682779694828469\n",
      "current epoch 368 ,  92.0 % completed\n",
      "LOSS: 0.599134087562561 ACCURCY: 0.6820097695852535\n",
      "current epoch 369 ,  92.25 % completed\n",
      "LOSS: 0.5993094444274902 ACCURCY: 0.6821064004096263\n",
      "current epoch 370 ,  92.5 % completed\n",
      "LOSS: 0.5755791664123535 ACCURCY: 0.6822644915514593\n",
      "current epoch 371 ,  92.75 % completed\n",
      "LOSS: 0.5946416258811951 ACCURCY: 0.6822744004096262\n",
      "current epoch 372 ,  93.0 % completed\n",
      "LOSS: 0.5951132774353027 ACCURCY: 0.6821403256528419\n",
      "current epoch 373 ,  93.25 % completed\n",
      "LOSS: 0.5858433842658997 ACCURCY: 0.68233808499744\n",
      "current epoch 374 ,  93.5 % completed\n",
      "LOSS: 0.5830570459365845 ACCURCY: 0.6824961597542243\n",
      "current epoch 375 ,  93.75 % completed\n",
      "LOSS: 0.5844627022743225 ACCURCY: 0.6829294377880183\n",
      "current epoch 376 ,  94.0 % completed\n",
      "LOSS: 0.6029645204544067 ACCURCY: 0.6819905499231951\n",
      "current epoch 377 ,  94.25 % completed\n",
      "LOSS: 0.5827319622039795 ACCURCY: 0.68235408499744\n",
      "current epoch 378 ,  94.5 % completed\n",
      "LOSS: 0.5915109515190125 ACCURCY: 0.6825649728622633\n",
      "current epoch 379 ,  94.75 % completed\n",
      "LOSS: 0.5872278213500977 ACCURCY: 0.6830049728622631\n",
      "current epoch 380 ,  95.0 % completed\n",
      "LOSS: 0.6065676808357239 ACCURCY: 0.6822600102406555\n",
      "current epoch 381 ,  95.25 % completed\n",
      "LOSS: 0.593901515007019 ACCURCY: 0.682705122375832\n",
      "current epoch 382 ,  95.5 % completed\n",
      "LOSS: 0.6027726531028748 ACCURCY: 0.6820283256528417\n",
      "current epoch 383 ,  95.75 % completed\n",
      "LOSS: 0.5950480103492737 ACCURCY: 0.682720176139273\n",
      "current epoch 384 ,  96.0 % completed\n",
      "LOSS: 0.5927138328552246 ACCURCY: 0.6828872135176651\n",
      "current epoch 385 ,  96.25 % completed\n",
      "LOSS: 0.5884473323822021 ACCURCY: 0.6823892882744496\n",
      "current epoch 386 ,  96.5 % completed\n",
      "LOSS: 0.5920101404190063 ACCURCY: 0.6824750476190476\n",
      "current epoch 387 ,  96.75 % completed\n",
      "LOSS: 0.6008937358856201 ACCURCY: 0.6828923256528416\n",
      "current epoch 388 ,  97.0 % completed\n",
      "LOSS: 0.5917075872421265 ACCURCY: 0.6814468817204302\n",
      "current epoch 389 ,  97.25 % completed\n",
      "LOSS: 0.5759276747703552 ACCURCY: 0.6823848069636456\n",
      "current epoch 390 ,  97.5 % completed\n",
      "LOSS: 0.5879938006401062 ACCURCY: 0.6820728069636455\n",
      "current epoch 391 ,  97.75 % completed\n",
      "LOSS: 0.5956644415855408 ACCURCY: 0.6821112135176652\n",
      "current epoch 392 ,  98.0 % completed\n",
      "LOSS: 0.5841173529624939 ACCURCY: 0.6816840102406555\n",
      "current epoch 393 ,  98.25 % completed\n",
      "LOSS: 0.5854602456092834 ACCURCY: 0.6822340849974399\n",
      "current epoch 394 ,  98.5 % completed\n",
      "LOSS: 0.5991483926773071 ACCURCY: 0.6820830476190474\n",
      "current epoch 395 ,  98.75 % completed\n",
      "LOSS: 0.5853144526481628 ACCURCY: 0.6820955289298515\n",
      "current epoch 396 ,  99.0 % completed\n",
      "LOSS: 0.6083031296730042 ACCURCY: 0.6819854541730669\n",
      "current epoch 397 ,  99.25 % completed\n",
      "LOSS: 0.5766753554344177 ACCURCY: 0.6823220849974398\n",
      "current epoch 398 ,  99.5 % completed\n",
      "LOSS: 0.6075704097747803 ACCURCY: 0.6830395289298514\n",
      "current epoch 399 ,  99.75 % completed\n",
      "LOSS: 0.5925694704055786 ACCURCY: 0.6823508817204302\n",
      "current epoch 400 ,  100.0 % completed\n"
     ]
    }
   ],
   "source": [
    "model_trainingInfo = trainMyModel(modelCarlos, criterion, train_loader, validation_loader, optimizer, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb26e7f2cc0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = int(len(model_trainingInfo[\"training_loss\"])/len(model_trainingInfo[\"validation_accuracy\"]))\n",
    "plt.plot(model_trainingInfo[\"training_loss\"][::steps], color = \"red\")\n",
    "plt.plot(model_trainingInfo[\"validation_accuracy\"], color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models//modelCarlos_2018.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN_Carlos(\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (drop2): Dropout(p=0.25, inplace=False)\n",
       "  (linear1_1): Linear(in_features=31, out_features=31, bias=True)\n",
       "  (linear2_1): Linear(in_features=31, out_features=10, bias=True)\n",
       "  (linear3_1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (linear1_2): Linear(in_features=31, out_features=31, bias=True)\n",
       "  (linear2_2): Linear(in_features=31, out_features=10, bias=True)\n",
       "  (linear3_2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (bn1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (out): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(modelCarlos.state_dict(), path)\n",
    "modelCarlos.load_state_dict(torch.load(path))\n",
    "modelCarlos.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
